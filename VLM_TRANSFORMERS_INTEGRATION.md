# ğŸ–¼ï¸ VLM Transformers Entegrasyon KÄ±lavuzu

## Genel BakÄ±ÅŸ

**Qwen2-VL-32B** modelini **Hugging Face transformers** kÃ¼tÃ¼phanesi ile lokal olarak Ã§alÄ±ÅŸtÄ±rÄ±yoruz:

- **Primary:** Transformers lokal inference (tablo/diagram analizi)
- **Fallback:** Hugging Face Inference Router (transformer baÅŸarÄ±sÄ±z olunca)

---

## Kurulum

### 1ï¸âƒ£ BaÄŸÄ±mlÄ±lÄ±klarÄ± YÃ¼kle

```bash
pip install -r vlm_transformers_requirements.txt
```

**Gerekli paketler:**
- `transformers>=4.36.0` â€” Model loading ve inference
- `torch>=2.1.0` â€” PyTorch (CUDA support)
- `bitsandbytes` â€” 8-bit quantization (opsiyonel, bellek tasarrufu)
- `accelerate` â€” Multi-GPU support
- `fastapi`, `uvicorn` â€” API server

### 2ï¸âƒ£ GPU Kontrol

```bash
nvidia-smi
```

**Gerekli:** En az 48 GB VRAM (A100, H100 vs.)
- **A100 40GB:** Quantization (8-bit) gerekli â†’ `USE_8BIT=true`
- **A100 80GB:** Native float16 â†’ `USE_8BIT=false`
- **CPU:** Desteklenmiyor (Ã§ok yavaÅŸ olur)

### 3ï¸âƒ£ Transformers Server BaÅŸlat

```bash
./start_vlm_transformers.sh
```

Veya manuel:
```bash
export LOCAL_VLM_MODEL="Qwen/Qwen2-VL-32B-Instruct"
export USE_8BIT=false
export HUGGINGFACE_API_KEY=$(grep HUGGINGFACE_API_KEY .env.local | cut -d= -f2)

python vlm_transformers_server.py
```

**Output Ã¶rneÄŸi:**
```
INFO:vlm:ğŸ“¦ Model yÃ¼kleniyor: Qwen/Qwen2-VL-32B-Instruct
INFO:vlm:   8-bit quantization: False
INFO:uvicorn:Uvicorn running on http://0.0.0.0:8001
```

---

## API KullanÄ±mÄ±

### Health Check

```bash
curl http://localhost:8001/health | jq .
```

**Response:**
```json
{
  "status": "healthy",
  "model": "Qwen/Qwen2-VL-32B-Instruct",
  "type": "transformers_local",
  "device": "cuda",
  "backend_ready": true
}
```

### GÃ¶rsel Analiz

```bash
# Ã–rnek gÃ¶rsel base64'e dÃ¶nÃ¼ÅŸtÃ¼r
IMAGE_B64=$(base64 -i /path/to/image.png)

# Request gÃ¶nder
curl -X POST http://localhost:8001/analyze \
  -H "Content-Type: application/json" \
  -d "{
    \"image_base64\": \"$IMAGE_B64\",
    \"task\": \"table\",
    \"language\": \"turkish\"
  }" | jq .
```

**Tasks:**
- `extract` â€” TABLO / DÄ°YAGRAM / GRAFIK / METIN kategorisine sÄ±nÄ±flandÄ±r
- `table` â€” Tablo olup olmadÄ±ÄŸÄ±nÄ± kontrol et, varsa Markdown formatÄ±nda gÃ¶ster
- `diagram` â€” Diyagram/ÅŸema analizÄ±
- `describe` â€” Genel aÃ§Ä±klama

**Response:**
```json
{
  "task": "table",
  "analysis": "| Lokanta | Kahvehane | Birahane |\n|---------|-----------|----------|\n| 255 | 31 | 760 |",
  "confidence": 0.92,
  "content_type": "table"
}
```

---

## Sistem Ä°ntegrasyonu

### PDF Upload AkÄ±ÅŸÄ±

1. **Upload route** (`app/api/rag/upload/route.ts`)
   - PDF sayfalarÄ±nÄ± render et
   - VLM server'a POST `/analyze`
   - SonuÃ§larÄ± dokÃ¼manlara ekle
   - Embedding oluÅŸtur
   - DB'ye kaydet

2. **VLM Server** (`vlm_transformers_server.py`)
   - Base64 gÃ¶rsel al
   - Transformers ile inference
   - Tablo/diagram/metin Ã§Ä±kart
   - JSON response dÃ¶ndÃ¼r

3. **Query endpoint** (`app/api/rag/query/route.ts`)
   - Vector search â†’ top 10 chunk
   - Reranker (Qwen3) â†’ best 3-5
   - LLM (GPT-4o-mini) â†’ answer
   - VLM chunks ayrÄ±ca baÄŸlam olarak eklenmiÅŸ

---

## Ortam DeÄŸiÅŸkenleri

**`.env.local` dosyasÄ±nda set et:**

```bash
# HuggingFace
HUGGINGFACE_API_KEY=hf_xxxxxxxx  # Fallback ve model download iÃ§in gerekli

# OpenAI (LLM)
OPENAI_API_KEY=sk-xxxxxxxx

# VLM Server
LOCAL_VLM_MODEL=Qwen/Qwen2-VL-32B-Instruct
USE_8BIT=false  # true â†’ 8-bit quantization
```

---

## Performans Ä°puÃ§larÄ±

### 1ï¸âƒ£ 8-bit Quantization (Bellek tasarrufu)

```bash
USE_8BIT=true ./start_vlm_transformers.sh
```

- Bellek: 65GB â†’ ~40GB
- HÄ±z: -5-10% (negligible)
- Kalite: Minimal loss

### 2ï¸âƒ£ Multi-GPU (DaÄŸÄ±tÄ±lmÄ±ÅŸ Ã§alÄ±ÅŸtÄ±rma)

Transformers `device_map="auto"` ile otomatik daÄŸÄ±tÄ±r. 2+ GPU iÃ§in:

```bash
# device_map="auto" zaten enable
python vlm_transformers_server.py
```

### 3ï¸âƒ£ Smaller Model (Test iÃ§in)

HÄ±zlÄ± prototype:

```bash
LOCAL_VLM_MODEL="microsoft/phi-2" ./start_vlm_transformers.sh
```

---

## Troubleshooting

### âŒ Model yÃ¼kleme baÅŸarÄ±sÄ±z

```
TransformersError: Can't connect to huggingface.co
```

**Ã‡Ã¶zÃ¼m:** HF offline mode:

```bash
export HF_DATASETS_OFFLINE=1
export HF_HUB_OFFLINE=1
# Fakat model daha Ã¶nce cache'e indirilmiÅŸ olmalÄ±
```

### âŒ CUDA out of memory

```
RuntimeError: CUDA out of memory
```

**Ã‡Ã¶zÃ¼m:**
1. 8-bit quantization aÃ§: `USE_8BIT=true`
2. Model boyutunu kÃ¼Ã§Ã¼lt
3. Batch size dÃ¼ÅŸÃ¼r (None varsa)

### âŒ Transformers yavaÅŸ, HF Router'a fal

```
âš ï¸ Transformers failed, fallback to HF Router
```

**Kontrol:**
- GPU kullanÄ±lÄ±yor mu: `nvidia-smi`
- Model cache'te mi: `~/.cache/huggingface/hub`
- Bellek yeterli mi: `nvidia-smi` â†’ free memory

---

## Lokal vs. HF Inference

| Kriter | Lokal Transformers | HF Router |
|--------|-------------------|-----------|
| **Latency** | ~2-3s (GPU) | ~3-5s (network) |
| **Cost** | GPU rental | API per-request |
| **Privacy** | TÃ¼m veri lokal | HF servers'a gidiyor |
| **Setup** | Complex | Basit |
| **Scaling** | Manual | Otomatik |

---

## Sonraki AdÄ±mlar

1. **VLM Server baÅŸlat:**
   ```bash
   ./start_vlm_transformers.sh
   ```

2. **PDF yÃ¼kle ve test et:**
   - UI â†’ Upload PDF
   - "Tablo 1'e gÃ¶re..." sorusunu sor
   - Tablo chunks DB'de kaydedilsin

3. **Query test:**
   ```bash
   curl -X POST http://localhost:3000/api/rag/query \
     -H "Content-Type: application/json" \
     -d '{
       "question": "Tablo 1 nedir?",
       "userId": "demo-user"
     }' | jq .
   ```

---

## Log DosyalarÄ±

```bash
# VLM server logs
tail -f vlm.log

# Next.js upload logs
tail -f next.log | grep "VLM"

# Reranker logs
tail -f reranker.log
```
