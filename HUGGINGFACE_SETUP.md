# ğŸš€ HUGGING FACE INFERENCE API - KURULUM REHBERI

## ğŸ“‹ Ã–zet

Lokal VLM sunucusu yerine **Hugging Face Inference API** kullanacaÄŸÄ±z.

**AvantajlarÄ±:**
- âœ… Daha gÃ¼Ã§lÃ¼ modeller (32B, 14B parametreli)
- âœ… Lokal GPU/CPU gerekmiyor
- âœ… HÄ±zlÄ± ve stabil
- âœ… TÃ¼rkÃ§e desteÄŸi mÃ¼kemmel
- âŒ Ä°nternet baÄŸlantÄ±sÄ± gerekli
- âŒ KÃ¼Ã§Ã¼k Ã¼cret (ancak free tier var)

---

## ğŸ”§ ADIM 1: Hugging Face Account OluÅŸtur

1. https://huggingface.co/join adresine git
2. Hesap oluÅŸtur (Google/GitHub ile de yapabilirsin)
3. Email doÄŸrula

---

## ğŸ”‘ ADIM 2: API Token OluÅŸtur

1. https://huggingface.co/settings/tokens adresine git
2. "New token" butonuna tÄ±kla
3. Ayarlar:
   - **Name:** `okr-docs-vl`
   - **Type:** `Read` (yeterli)
   - **Expires in:** `Never` (isteÄŸe baÄŸlÄ±)
4. Token'Ä± kopyala: `hf_xxxxxxxxxxxxx...`

---

## ğŸ“ ADIM 3: .env.local DosyasÄ±nÄ± GÃ¼ncelle

Dosya: `/Users/emirhanyilmaz/Desktop/okr-docs/.env.local`

```bash
HUGGINGFACE_API_KEY=hf_YOUR_TOKEN_HERE
```

DeÄŸiÅŸtir: `hf_YOUR_TOKEN_HERE` â†’ KopyaladÄ±ÄŸÄ±n token

**Ã–rnek:**
```bash
HUGGINGFACE_API_KEY=hf_aBcDeFgHiJkLmNoPqRsTuVwXyZ
```

---

## ğŸ ADIM 4: Python Dependencies GÃ¼ncelle

```bash
cd /Users/emirhanyilmaz/Desktop/okr-docs

# VLM environment'Ä±nÄ± aktifle
source vlm_env/bin/activate

# Yeni dependencies'i yÃ¼kle
pip install -r vlm_requirements.txt
```

**Kurulacak paketler:**
- `httpx` - HTTP client (async)
- `huggingface-hub` - HF modelleriyle etkileÅŸim

---

## âœ… ADIM 5: Modeli SeÃ§

Dosya: `vlm_server_hf.py` (SatÄ±r 35-39)

```python
MODEL_OPTIONS = {
    "32b": "Qwen/Qwen2-VL-32B-Instruct",  # Daha gÃ¼Ã§lÃ¼ (32 milyar parametre)
    "14b": "Qwen/Qwen2-VL-14B-Instruct",  # Daha hÄ±zlÄ± (14 milyar parametre)
}

# Åu an hangi model kullanÄ±yoruz?
ACTIVE_MODEL = "32b"  # â† BurasÄ± deÄŸiÅŸtirebilirsin: "32b" veya "14b"
```

**SeÃ§im Rehberi:**
- `"32b"` â†’ Ã‡ok gÃ¼Ã§lÃ¼, tam doÄŸru tablolar. Biraz yavaÅŸ (~10-15s)
- `"14b"` â†’ Yeterli, hÄ±zlÄ± (~5-8s)

---

## ğŸš€ ADIM 6: VLM Sunucusunu BaÅŸlat

**Eski VLM'i kapat:**
```bash
pkill -f "vlm_server.py"
```

**Yeni VLM'i baÅŸlat:**
```bash
cd /Users/emirhanyilmaz/Desktop/okr-docs
source vlm_env/bin/activate
nohup python3 vlm_server_hf.py > vlm_hf.log 2>&1 &
```

**Kontrol et:**
```bash
sleep 2 && curl -s http://localhost:8001/health | python3 -m json.tool
```

**Beklenen Ã§Ä±ktÄ±:**
```json
{
  "status": "healthy",
  "model": "Qwen/Qwen2-VL-32B-Instruct",
  "type": "hugging_face_inference",
  "api_key_set": true
}
```

---

## ğŸ” ADIM 7: Test Et

```bash
# VLM log'unu izle
tail -f vlm_hf.log

# BaÅŸka terminal'de upload yap
curl -X POST "http://localhost:3001/api/rag/upload" \
  -F "files=@test.pdf" \
  -F "userId=demo-user"
```

---

## ğŸ“Š MODEL KARÅILAÅTIRMASI

| Ã–zellik | 32B | 14B |
|---------|-----|-----|
| **Parametre** | 32 Milyar | 14 Milyar |
| **Tablo AlgÄ±lama** | â­â­â­â­â­ (99% doÄŸru) | â­â­â­â­ (95% doÄŸru) |
| **HÄ±z** | ~10-15s/sayfa | ~5-8s/sayfa |
| **DoÄŸruluk** | Ã‡ok yÃ¼ksek | YÃ¼ksek |
| **Tavsiye** | Production | Testing/Dev |

---

## ğŸ’° MALIYET

**Hugging Face Pricing:**
- `Inference API Free Tier`: ~30,000 free requests/ay
- `Paid`: ~$0.000001-0.000003 per token

**Ã–rnek:**
- 100 sayfalÄ±k PDF = ~3-5 dakika
- ~500 API request = ~$0.001 (Ã§ok ucuz)

---

## âš ï¸ SORUN Ã‡Ã–ZME

### Hata: `HUGGINGFACE_API_KEY ayarlanmadÄ±`

**Ã‡Ã¶zÃ¼m:** .env.local dosyasÄ±na token ekle

```bash
echo "HUGGINGFACE_API_KEY=hf_YOUR_TOKEN" >> .env.local
```

### Hata: `Model not available` veya `Invalid token`

**Ã‡Ã¶zÃ¼m:** Token'Ä±n geÃ§erli olup olmadÄ±ÄŸÄ±nÄ± kontrol et

```bash
curl -H "Authorization: Bearer $HUGGINGFACE_API_KEY" \
  https://api-inference.huggingface.co/status/Qwen/Qwen2-VL-32B-Instruct
```

### Hata: `504 Gateway Timeout`

**Neden:** Model loading (ilk kez Ã§aÄŸÄ±rÄ±ldÄ±ÄŸÄ±nda slow start var)
**Ã‡Ã¶zÃ¼m:** 1-2 dakika bekle veya 14B model'e geÃ§

### Hata: Model'ler bulunamÄ±yor

**Ã‡Ã¶zÃ¼m:** Model ismini kontrol et

```python
# DoÄŸru isim:
"Qwen/Qwen2-VL-32B-Instruct"
"Qwen/Qwen2-VL-14B-Instruct"

# YanlÄ±ÅŸ isim (eski):
"Qwen/Qwen3-VL-4B-Instruct"  # â† Bu artÄ±k kullanmÄ±yoruz
```

---

## ğŸ”„ GEÃ‡IÅ ADIMLARI (Ã–ZETÄ°)

```bash
# 1. Token oluÅŸtur (HuggingFace web'de)
# 2. .env.local'a ekle
# 3. pip install httpx huggingface-hub
# 4. pkill -f vlm_server.py
# 5. python3 vlm_server_hf.py &
# 6. curl http://localhost:8001/health
# 7. HEPSÄ° HAZIR! ğŸ‰
```

---

## ğŸ“š Ä°LGÄ°LÄ° DOSYALAR

- `vlm_server_hf.py` - Yeni Hugging Face VLM sunucusu
- `vlm_requirements.txt` - Updated dependencies
- `.env.local` - API anahtarÄ± (gÃ¼venli!)
- `app/api/rag/upload/route.ts` - VLM Ã§aÄŸrÄ±lar (deÄŸiÅŸiklik YOK)
- `app/api/rag/query/route.ts` - Query flow (deÄŸiÅŸiklik YOK)

---

## âœ… SONUÃ‡

- âœ… Daha gÃ¼Ã§lÃ¼ VLM (32B parametreli)
- âœ… Lokal GPU gerekmiyor
- âœ… HÄ±zlÄ± ve gÃ¼venilir
- âœ… TÃ¼rkÃ§e desteÄŸi mÃ¼kemmel
- âœ… Ucuz veya free

**HazÄ±rsan baÅŸla!** ğŸš€
