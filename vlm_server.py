#!/usr/bin/env python3
"""
ğŸ–¼ï¸ QWEN3 VLM SERVER
Qwen/Qwen3-VL-4B-Instruct modeli ile gÃ¶rselleri analiz eden FastAPI server
PDF sayfalarÄ±ndan tablo, diagram, grafik Ã§Ä±karÄ±mÄ± yapÄ±yor
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import torch
import base64
from io import BytesIO
from PIL import Image, ImageDraw, ImageFilter
import logging
from transformers import AutoProcessor, Qwen2VLForConditionalGeneration
import pytesseract
import numpy as np

# Logging ayarla
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Qwen3 VLM Server", version="1.0")

# Global model ve processor
model = None
processor = None
device = None

class VLMRequest(BaseModel):
    """VLM analiz isteÄŸi"""
    image_base64: str  # Base64 encoded image
    task: str = "extract"  # "extract", "describe", "table", "diagram"
    language: str = "turkish"  # "turkish", "english", "mixed"

class VLMResponse(BaseModel):
    """VLM analiz yanÄ±tÄ±"""
    task: str
    analysis: str
    confidence: float
    content_type: str  # "text", "table", "diagram", "chart", "mixed"

@app.on_event("startup")
async def load_model():
    """Sunucu baÅŸlatÄ±ldÄ±ÄŸÄ±nda model yÃ¼kle"""
    global model, processor, device
    
    logger.info("ğŸ–¼ï¸ Qwen3-VL-4B-Instruct model yÃ¼kleniyor...")
    
    # Device seÃ§
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ğŸ“ Device: {device}")
    
    model_name = "Qwen/Qwen3-VL-4B-Instruct"
    try:
        # Processor ve model yÃ¼kle
        processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)
        model = Qwen2VLForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if device.type == "cuda" else torch.float32,
            attn_implementation="flash_attention_2" if device.type == "cuda" else "eager",
            device_map="auto" if device.type == "cuda" else None
        )
        
        if device.type == "cpu":
            model = model.to(device)
        
        model.eval()
        logger.info("âœ… VLM Model baÅŸarÄ±yla yÃ¼klendi")
    except Exception as e:
        logger.error(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
        raise

@app.post("/analyze", response_model=VLMResponse)
async def analyze_image(request: VLMRequest) -> VLMResponse:
    """
    GÃ¶rsel analiz yap - tablo, diagram, metin Ã§Ä±kar
    
    Args:
        request.image_base64: Base64 encoded gÃ¶rsel
        request.task: Ne yapÄ±lacak (extract, describe, table, diagram)
        request.language: Hangi dilde sonuÃ§ istediÄŸimiz
    
    Returns:
        VLMResponse: Analiz sonucu
    """
    if not model or not processor:
        raise HTTPException(status_code=500, detail="Model yÃ¼klenmedi")
    
    try:
        logger.info(f"ğŸ–¼ï¸ GÃ¶rsel analizi baÅŸladÄ± (task={request.task})")
        
        # Base64'ten gÃ¶rsele dÃ¶nÃ¼ÅŸtÃ¼r
        image_data = base64.b64decode(request.image_base64)
        image = Image.open(BytesIO(image_data)).convert("RGB")
        
        # GÃ¶rev spesifik prompt'lar - Ä°Ã§erik TÃœRÃœNÃœ tespit et
        prompts = {
            "extract": "Bu gÃ¶rselde tablo var mÄ±? Diyagram var mÄ±? Grafik var mÄ±? Sadece ÅŸu cevaplardan birini ver: 'TABLO', 'DIYAGRAM', 'GRAFIK', 'METIN'. BaÅŸka birÅŸey yazma!",
            "describe": "Bu gÃ¶rseli kÄ±saca aÃ§Ä±kla. Ne gÃ¶rmektedir? TÃ¼rkÃ§e olarak cevap ver.",
            "table": "Bu gÃ¶rselde tablo var mÄ±? Varsa tablo iÃ§eriÄŸini Markdown formatÄ±nda gÃ¶ster. TÃ¼rkÃ§e olarak cevap ver.",
            "diagram": "Bu gÃ¶rselde diyagram, grafik veya ÅŸekil var mÄ±? Varsa ne anlattÄ±ÄŸÄ±nÄ± aÃ§Ä±kla. TÃ¼rkÃ§e olarak cevap ver.",
        }
        
        prompt = prompts.get(request.task, prompts["extract"])
        
        # Modeli Ã§alÄ±ÅŸtÄ±r
        with torch.no_grad():
            # GÃ¶rseli ve prompt'u processor'a gÃ¶nder
            inputs = processor(
                text=prompt,
                images=[image],
                return_tensors="pt"
            ).to(device)
            
            # Model inference
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.1,  # Deterministik cevap
                top_p=0.95,
            )
            
            # Sonucu decode et
            analysis = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
        
        # VLM sonucundan iÃ§erik tÃ¼rÃ¼nÃ¼ Ã§Ä±kar
        analysis_lower = analysis.lower()
        
        # Ä°Ã§erik tÃ¼rÃ¼nÃ¼ belirle (VLM hatasÄ±na karÅŸÄ± fallback)
        if "tablo" in analysis_lower:
            content_type = "table"
            # Tabloyu OCR ile Ã§Ä±kar
            try:
                ocr_text = pytesseract.image_to_string(image, lang='tur+eng')
                analysis = f"[TABLO]\n\n{ocr_text}\n\n[VLM AÃ§Ä±klamasÄ±]\n{analysis}"
            except:
                pass
        elif "diyagram" in analysis_lower or "ÅŸema" in analysis_lower:
            content_type = "diagram"
        elif "grafik" in analysis_lower or "chart" in analysis_lower or "grafik" in analysis_lower:
            content_type = "chart"
        else:
            # Fallback: OCR ile metin Ã§Ä±kar
            content_type = "text"
            try:
                ocr_text = pytesseract.image_to_string(image, lang='tur+eng')
                if ocr_text.strip():
                    analysis = f"{ocr_text}\n\n[VLM AÃ§Ä±klamasÄ±]\n{analysis}"
            except:
                pass
        
        logger.info(f"âœ… Analiz tamamlandÄ± (type={content_type})")
        logger.info(f"   SonuÃ§: {analysis[:100]}...")
        
        return VLMResponse(
            task=request.task,
            analysis=analysis,
            confidence=0.90,
            content_type=content_type
        )
        
    except Exception as e:
        logger.error(f"âŒ Analiz hatasÄ±: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Analiz hatasÄ±: {str(e)}")

@app.get("/health")
async def health():
    """Sunucu saÄŸlÄ±k kontrolÃ¼"""
    return {
        "status": "healthy",
        "model": "Qwen/Qwen3-VL-4B-Instruct",
        "device": str(device),
        "model_loaded": model is not None
    }

@app.get("/")
async def root():
    """Ana sayfa"""
    return {
        "name": "Qwen3 VLM Server",
        "version": "1.0",
        "endpoints": [
            "/analyze (POST) - GÃ¶rsel analiz et",
            "/health (GET) - Sunucu durumu"
        ]
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
