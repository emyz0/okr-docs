# ğŸ¤– **AI MODELLERI - KAPSAMLI AÃ‡IKLAMA**

Bu dokÃ¼manda sistemde kullanÄ±lan tÃ¼m AI modellerinin Ã¶zellikleri, kullanÄ±mÄ±, ve kodu aÃ§Ä±klanmÄ±ÅŸtÄ±r.

---

## ğŸ“Š **Modeller Ã–zet Tablosu**

| Model | KÃ¼tÃ¼phane | Port | GÃ¶rev | Girdi | Ã‡Ä±ktÄ± |
|-------|-----------|------|-------|-------|-------|
| **OpenAI text-embedding-3-small** | LangChain | API | Metin â†’ Vector | Text (string) | Vector (1536-dim) |
| **OpenAI gpt-4o-mini** | LangChain | API | Soru Cevaplama | Prompt (text) | Response (text) |
| **Qwen3-VL-4B-Instruct** | Transformers | 8001 | GÃ¶rsel Analiz | Image + Prompt | Analysis (text) |
| **Qwen3-Reranker-4B** | Transformers | 8000 | Dokuman SÄ±ralama | Query + Docs | Ranked scores |

---

# ğŸŒŸ **1. OpenAI text-embedding-3-small**

## **Model Nedir?**

Metni sayÄ±sal vektÃ¶re (embedding) dÃ¶nÃ¼ÅŸtÃ¼ren model. Benzer metinler benzer vektÃ¶rler Ã¼retir.

```
"Veri tabanÄ±" â†’ [0.123, 0.456, 0.789, ..., 0.342]  (1536 sayÄ±)
"Database"    â†’ [0.124, 0.455, 0.791, ..., 0.341]  (benzer!)
"Trigonometry" â†’ [0.923, 0.102, 0.234, ..., 0.891] (Ã§ok farklÄ±)
```

## **Ã–zellikleri**

| Ã–zellik | DeÄŸer |
|---------|-------|
| **Model AdÄ±** | `text-embedding-3-small` |
| **Ã‡Ä±ktÄ± Boyutu** | 1536 dimensions |
| **Provider** | OpenAI |
| **HÄ±z** | ~50ms per text |
| **Maliyet** | Uygun (small model) |
| **Dil DesteÄŸi** | 100+ dil (TÃ¼rkÃ§e dahil) |
| **Max Token** | ~8000 characters |

## **Koddaki KullanÄ±mÄ±**

### **YÃ¼kleme** (`lib/rag/chain.ts`)
```typescript
import { OpenAIEmbeddings } from "@langchain/openai";

export const embeddings = new OpenAIEmbeddings({
  apiKey: process.env.OPENAI_API_KEY!,
  modelName: "text-embedding-3-small",  // â† Model adÄ±
});
```

### **Metin Embedding'i** (`app/api/rag/upload/route.ts`)
```typescript
// Her chunk'Ä± vektÃ¶re Ã§evir
const textSplitter = new RecursiveCharacterTextSplitter({
  chunkSize: 1000,      // Her parÃ§a 1000 char
  chunkOverlap: 200,    // Ã‡akÄ±ÅŸma 200 char
});

const chunks = await textSplitter.splitText(fullText);

// Her chunk'Ä± embed et
for (const chunk of chunks) {
  const embedding = await embeddings.embedQuery(chunk);
  // embedding = [0.123, 0.456, ..., 0.342] (1536 sayÄ±)
  
  // Database'e kaydet
  await pool.query(
    'INSERT INTO documents (embedding, content, ...) VALUES ($1, $2, ...)',
    [embedding, chunk, ...]  // â† VektÃ¶r burada
  );
}
```

### **Soru Embedding'i** (`app/api/rag/query/route.ts`)
```typescript
// KullanÄ±cÄ±nÄ±n sorusunu embed et
const questionEmbedding = await embeddings.embedQuery(question);
// questionEmbedding = [0.111, 0.222, ..., 0.333] (1536 sayÄ±)

// PostgreSQL'de vector similarity search yap
const results = await pool.query(`
  SELECT * FROM documents
  WHERE user_id = $1
  ORDER BY embedding <-> $2::vector  -- â† pgvector distance operator
  LIMIT 10
`, [userId, JSON.stringify(questionEmbedding)]);

// En benzer 10 chunk dÃ¶ner
```

## **Vector vs Cosine Similarity**

```
VektÃ¶r 1: "Veri tabanÄ±"    = [0.1, 0.9, 0.2, ...]
VektÃ¶r 2: "Database"       = [0.1, 0.8, 0.2, ...]  â† Benzer!
VektÃ¶r 3: "Trigonometry"   = [0.9, 0.1, 0.8, ...]  â† Ã‡ok farklÄ±

Cosine Similarity:
- VektÃ¶r 1 vs 2: 0.99 (benzer)
- VektÃ¶r 1 vs 3: 0.15 (farklÄ±)

pgvector <-> operator = Euclidean distance
(KÃ¼Ã§Ã¼k distance = benzer)
```

## **Ã–zel Parametreler**

```typescript
new OpenAIEmbeddings({
  apiKey: process.env.OPENAI_API_KEY!,    // â† API key zorunlu
  modelName: "text-embedding-3-small",     // â† Model seÃ§imi
  // DiÄŸer parametreler:
  // stripNewLines: true,                   // Newline'larÄ± kaldÄ±r
  // timeout: 60000,                        // 60 saniye timeout
  // maxRetries: 3,                         // 3 kez retry
});
```

## **Ne Ä°ÅŸe Yarar?**

1. **Benzer Metinleri Bulma** - "Database"yi sorsam, "Veri tabanÄ±" bulur
2. **Vector Search** - HÄ±zlÄ± arama (pgvector ile)
3. **RAG'Ä±n Kalbi** - TÃ¼m sistem bu embedding'e baÄŸlÄ±

---

# ğŸ§  **2. OpenAI gpt-4o-mini**

## **Model Nedir?**

BÃ¼yÃ¼k Dil Modeli (LLM). Soruya doÄŸal dil cevabÄ± verir.

```
Ä°nput:  "Veri tabanÄ± nedir? Kaynaklar: [context]"
Output: "Veri tabanÄ±, yapÄ±landÄ±rÄ±lmÄ±ÅŸ verilerin depolanmasÄ±..."
```

## **Ã–zellikleri**

| Ã–zellik | DeÄŸer |
|---------|-------|
| **Model AdÄ±** | `gpt-4o-mini` |
| **Ailem** | GPT-4 (gÃ¼Ã§lÃ¼) |
| **HÄ±z** | ~2-5 saniye per query |
| **Maliyet** | Uygun (mini variant) |
| **Context Penceresi** | ~128K tokens (~100K words) |
| **Ã‡Ä±ktÄ±** | Natural language text |
| **Temperature** | 0.1 (deterministic) |

## **Koddaki KullanÄ±mÄ±**

### **YÃ¼kleme** (`lib/rag/chain.ts`)
```typescript
import { ChatOpenAI } from "@langchain/openai";

export const llm = new ChatOpenAI({
  apiKey: process.env.OPENAI_API_KEY!,
  modelName: "gpt-4o-mini",      // â† Model adÄ±
  temperature: 0.1,              // â† Deterministic cevaplar
});
```

**Temperature AÃ§Ä±klamasÄ±:**
- `0.0` = HiÃ§ rastgele, her zaman aynÄ± cevap
- `0.5` = Orta rastgelelik
- `1.0` = Ã‡ok rastgele, yaratÄ±cÄ± cevaplar
- **Bizim: 0.1** = TutarlÄ±, faktual cevaplar (RAG iÃ§in ideal)

### **LLM Ã‡aÄŸrÄ±sÄ±** (`app/api/rag/query/route.ts`)
```typescript
// Prompt hazÄ±rla
const prompt = `
System: Sen bir asistan'sÄ±n. KullanÄ±cÄ±nÄ±n sorusunu kaynaklarÄ± kullanarak cevapla.

KAYNAKLAR:
${contextText}

SORU: ${question}

CEVAP:
`;

// LLM'e gÃ¶nder
const response = await llm.invoke(prompt);
// response.content = "Cevap metni..."

const answer = response.content as string;

// CevabÄ± ve kaynaklarÄ± dÃ¶ndÃ¼r
return {
  answer: answer,
  sources: rankedDocs,
  sectionId: sectionId,
};
```

## **Prompt Engineering**

```typescript
// âŒ KÃ–TÃœ PROMPT
"Tablo 1'deki veriler nelerdir?"

// âœ… Ä°YÄ° PROMPT (Bizim kullandÄ±ÄŸÄ±mÄ±z)
`
System: Sen bir RAG asistanÄ±'sÄ±n. Verilen kaynaklarÄ± kullanarak cevapla. 
EÄŸer cevap kaynaklarda yoksa "Bilmiyorum" de.

KAYNAKLAR:
[DÃ¶kÃ¼manlar]

SORU: Tablo 1'deki veriler nelerdir?

Cevap:
`

// âœ… DAHA Ä°YÄ° (Hiperparametre ayarlanmÄ±ÅŸ)
`
System: Rol: Teknik dokÃ¼man asistanÄ±
Ton: Profesyonel, aÃ§Ä±klayÄ±cÄ±
Dil: TÃ¼rkÃ§e

Verilen kaynaklarÄ± SADECE kullan. BunlarÄ±n dÄ±ÅŸÄ±nda bilgi ekleme.

KAYNAKLAR:
[DÃ¶kÃ¼manlar]

SORU: ${question}

Format: CevabÄ± madde madde ver. Kaynak referanslarÄ± ekle.

CEVAP:
`
```

## **Ã–zel Parametreler**

```typescript
new ChatOpenAI({
  apiKey: process.env.OPENAI_API_KEY!,
  modelName: "gpt-4o-mini",
  temperature: 0.1,               // â† Deterministic
  // DiÄŸer parametreler:
  // maxTokens: 2000,              // Max Ã§Ä±ktÄ± 2000 token
  // topP: 0.95,                   // Nucleus sampling
  // presencePenalty: 0,           // Tekrar cezasÄ±
  // frequencyPenalty: 0,          // SÄ±klÄ±k cezasÄ±
  // timeout: 60000,               // 60 saniye timeout
});
```

## **Ne Ä°ÅŸe Yarar?**

1. **Soru Cevaplama** - Context'e gÃ¶re cevap oluÅŸturur
2. **RAG'Ä±n Beyin'i** - TÃ¼m reasoning burada
3. **Conversation History** - GeÃ§miÅŸ sohbetleri hatÄ±rlar

---

# ğŸ‘ï¸ **3. Qwen3-VL-4B-Instruct**

## **Model Nedir?**

Vision Language Model (VLM). GÃ¶rselleri anlayÄ±p metin aÃ§Ä±klamasÄ± oluÅŸturur.

```
Ä°nput:  GÃ¶rsel (PDF sayfasÄ±, tablo, grafik)
Output: "Bu tablo ÅŸu verileri iÃ§eriyor: ... Grafik trendi gÃ¶steriyor..."
```

## **Ã–zellikleri**

| Ã–zellik | DeÄŸer |
|---------|-------|
| **Model AdÄ±** | `Qwen/Qwen3-VL-4B-Instruct` |
| **Aile** | Qwen Vision Language Models |
| **Parametre** | 4 Billion |
| **TÃ¼rÃ¼** | Instruction-tuned (sorulara cevap verir) |
| **Ã‡Ä±ktÄ± Boyutu** | ~1536 tokens max |
| **HÄ±z** | ~3-5 saniye per page (CPU'da) |
| **Girdi** | Image (Base64) + Text Prompt |
| **Diller** | Multilingual (TÃ¼rkÃ§e dahil) |

## **Koddaki KullanÄ±mÄ±**

### **YÃ¼kleme** (`vlm_server.py`)
```python
from transformers import AutoProcessor, Qwen3VLForConditionalGeneration
import torch

# Device seÃ§ (GPU veya CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# â†’ CPU kullanÄ±yor (CUDA yok)

# Model yÃ¼kle
processor = AutoProcessor.from_pretrained(
    "Qwen/Qwen3-VL-4B-Instruct",
    trust_remote_code=True  # â† Custom model iÃ§in gerekli
)

model = Qwen3VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen3-VL-4B-Instruct",
    torch_dtype=torch.float32,  # CPU'da float32 (GPU'da float16)
    attn_implementation="eager",  # CPU'da eager (GPU'da flash_attention_2)
    device_map=None,  # CPU'da None
    trust_remote_code=True
)

model = model.to(device)
model.eval()  # Evaluation mode
```

### **GÃ¶rsel Analizi** (`vlm_server.py` - POST /analyze)
```python
@app.post("/analyze")
async def analyze_image(request: VLMRequest):
    # Request formatÄ±:
    # {
    #   "image_base64": "iVBORw0KGgoAAAANS...",
    #   "task": "extract",
    #   "language": "turkish"
    # }
    
    # Base64'ten gÃ¶rsele dÃ¶nÃ¼ÅŸtÃ¼r
    image_data = base64.b64decode(request.image_base64)
    image = Image.open(BytesIO(image_data)).convert("RGB")
    
    # Task'a gÃ¶re prompt seÃ§
    prompts = {
        "extract": "Bu gÃ¶rselde tablo var mÄ±? Diyagram var mÄ±? Sadece cevap: TABLO, DIYAGRAM, GRAFIK, METIN",
        "describe": "Bu gÃ¶rseli detaylÄ± aÃ§Ä±kla. TÃ¼rkÃ§e olarak cevap ver.",
        "table": "Tablo var mÄ±? Varsa iÃ§eriÄŸini Markdown'da gÃ¶ster.",
        "diagram": "Diyagram var mÄ±? Varsa ne anlattÄ±ÄŸÄ±nÄ± aÃ§Ä±kla.",
    }
    
    prompt = prompts.get(request.task, prompts["extract"])
    
    # Model'e gÃ¶nder
    with torch.no_grad():  # Gradyan hesaplama = OFF (inference iÃ§in)
        inputs = processor(
            text=prompt,              # â† Text prompt
            images=[image],           # â† GÃ¶rsel
            return_tensors="pt"       # PyTorch tensor olarak
        ).to(device)
        
        # Inference yap
        generated_ids = model.generate(
            **inputs,
            max_new_tokens=512,       # Max 512 token Ã§Ä±ktÄ±
            temperature=0.1,          # Deterministic
            top_p=0.95,               # Nucleus sampling
        )
        
        # Sonucu decode et
        analysis = processor.batch_decode(
            generated_ids, 
            skip_special_tokens=True
        )[0]
    
    # Ä°Ã§erik tÃ¼rÃ¼nÃ¼ belirle
    if "tablo" in analysis.lower():
        content_type = "table"
    elif "diyagram" in analysis.lower():
        content_type = "diagram"
    else:
        content_type = "text"
    
    return VLMResponse(
        task=request.task,
        analysis=analysis,
        confidence=0.90,
        content_type=content_type
    )
```

### **Upload Pipeline'da KullanÄ±mÄ±** (`app/api/rag/upload/route.ts`)
```typescript
// 1. PDF sayfalarÄ±nÄ± render et
for (let i = 1; i <= maxPages; i++) {
  // SayfayÄ± Base64 gÃ¶rsele Ã§evir
  const base64Image = await renderPdfPageToBase64(pdfPath, i);
  
  // VLM'e HTTP isteÄŸi gÃ¶nder
  const vlmResponse = await fetch('http://localhost:8001/analyze', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      image_base64: base64Image,      // â† Base64 gÃ¶rsel
      task: 'extract',                 // â† Ã‡Ä±kartma gÃ¶revi
      language: 'turkish'              // â† TÃ¼rkÃ§e Ã§Ä±ktÄ±
    })
  });
  
  const vlmResult = await vlmResponse.json();
  // {
  //   analysis: "Bu tablo ÅŸu verileri iÃ§eriyor...",
  //   content_type: "table",
  //   confidence: 0.90
  // }
  
  // Sonucu chunks'a dÃ¶nÃ¼ÅŸtÃ¼r
  vlmChunks.push({
    content: `[TABLO - Sayfa ${i}]\n\n${vlmResult.analysis}`,
    metadata: {
      source: filename,
      page: i,
      contentType: vlmResult.content_type,
      has_images: true
    }
  });
}
```

## **Ã–zel Parametreler**

```python
# generate() parametreleri
generated_ids = model.generate(
    **inputs,
    max_new_tokens=512,        # Maximum output length
    temperature=0.1,           # Deterministic (0 = hiÃ§ randomluk)
    top_p=0.95,               # Nucleus sampling (top %95 prob)
    # DiÄŸer parametreler:
    # do_sample=True,          # Sampling kullan (default)
    # repetition_penalty=1.2,  # Tekrar cezasÄ±
    # early_stopping=True,     # Stop when done
    # num_beams=1,             # Beam search (1 = greedy)
)

# Processor parametreleri
processor(
    text=prompt,              # Text instruction
    images=[image],           # List of images
    return_tensors="pt",      # PyTorch tensors
    # DiÄŸer parametreler:
    # padding=True,            # Pad sequences
    # truncation=True,         # Truncate long sequences
    # max_length=1024,         # Max input length
)
```

## **Ne Ä°ÅŸe Yarar?**

1. **Tablo Analizi** - Tablo iÃ§eriÄŸini anlatÄ±r
2. **Diagram AÃ§Ä±klama** - ÅemalarÄ±, grafikleri aÃ§Ä±klar
3. **GÃ¶rsel SÄ±nÄ±flandÄ±rma** - Bu sayfa tablo mÄ±, diagram mÄ±, metin mi?
4. **OCR'dan ÃœstÃ¼n** - Sadece metin deÄŸil, yapÄ±yÄ± anlar

---

# ğŸ¯ **4. Qwen3-Reranker-4B**

## **Model Nedir?**

Dokuman sÄ±ralama modeli. Top 10 dÃ¶kÃ¼manÄ±, relevance'e gÃ¶re sÄ±ralar.

```
Ä°nput:  Query: "Tablo 1'deki veriler?"
        Documents: [doc1, doc2, doc3, ...]
        
Output: Top 3 dÃ¶kÃ¼man (relevance score ile)
        doc3: 0.95 (en relevant)
        doc7: 0.87
        doc2: 0.76
```

## **Ã–zellikleri**

| Ã–zellik | DeÄŸer |
|---------|-------|
| **Model AdÄ±** | `Qwen/Qwen3-Reranker-4B` |
| **Aile** | Qwen Reranker |
| **Parametre** | 4 Billion |
| **GÃ¶revi** | Relevance ranking |
| **HÄ±z** | ~500ms per batch (CPU'da) |
| **Girdi** | Query + Dokuman listesi |
| **Ã‡Ä±ktÄ±** | Relevance scores (0-1) |

## **Koddaki KullanÄ±mÄ±**

### **YÃ¼kleme** (`reranker_server.py`)
```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# â†’ CPU kullanÄ±yor

tokenizer = AutoTokenizer.from_pretrained(
    "Qwen/Qwen3-Reranker-4B",
    trust_remote_code=True
)

model = AutoModelForSequenceClassification.from_pretrained(
    "Qwen/Qwen3-Reranker-4B",
    torch_dtype=torch.float32,  # CPU'da float32
    device_map=None,
    trust_remote_code=True
)

model = model.to(device)
model.eval()
```

### **Reranking** (`reranker_server.py` - POST /rerank)
```python
@app.post("/rerank")
async def rerank(request: RerankerRequest):
    # Request formatÄ±:
    # {
    #   "query": "Tablo 1'deki veriler?",
    #   "documents": [
    #     "Tablo 1: SatÄ±ÅŸ verileri: ...",
    #     "Grafik 2: Trend gÃ¶steriyor...",
    #     "Metin 3: AÃ§Ä±klama...",
    #     ...
    #   ],
    #   "top_k": 3
    # }
    
    query = request.query
    documents = request.documents
    top_k = request.top_k
    
    # Rerank iÃ§in input hazÄ±rla
    # Format: "query [SEP] document"
    pairs = [
        f"{query} [SEP] {doc}" 
        for doc in documents
    ]
    
    # Tokenize
    with torch.no_grad():
        inputs = tokenizer(
            pairs,
            padding=True,           # Pad to same length
            truncation=True,        # Truncate long sequences
            return_tensors="pt",    # PyTorch tensors
            max_length=512          # Max input length
        ).to(device)
        
        # Model inference
        logits = model(**inputs).logits
        # logits.shape = (batch_size, 1)
        
        # Softmax â†’ probabilities
        scores = torch.softmax(logits, dim=-1)[:, 1]
        # scores.shape = (batch_size,)  [0.1, 0.95, 0.7, ...]
    
    # SÄ±ralama
    ranked_indices = torch.argsort(scores, descending=True)[:top_k]
    
    # SonuÃ§
    ranked_documents = [
        {
            "index": int(idx),
            "score": float(scores[idx]),
            "document": documents[int(idx)]
        }
        for idx in ranked_indices
    ]
    
    return {
        "query": query,
        "ranked_documents": ranked_documents,
        "top_k": top_k
    }
```

### **Query Pipeline'da KullanÄ±mÄ±** (`app/api/rag/query/route.ts`)
```typescript
// 1. Vector search: Top 10 dÃ¶kÃ¼man bul
const searchResults = await pool.query(`
  SELECT * FROM documents
  WHERE user_id = $1
  ORDER BY embedding <-> $2::vector
  LIMIT 10
`, [userId, JSON.stringify(questionEmbedding)]);

const topDocuments = searchResults.rows;  // 10 dÃ¶kÃ¼man

// 2. Reranker'a gÃ¶nder
const rerankerResponse = await fetch('http://localhost:8000/rerank', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    query: question,                          // â† Query
    documents: topDocuments.map(d => d.content),  // â† Top 10 docs
    top_k: 3                                  // â† En iyi 3 iste
  })
});

const rerankerResult = await rerankerResponse.json();
// {
//   ranked_documents: [
//     { index: 5, score: 0.92, document: "..." },  // En relevant
//     { index: 2, score: 0.87, document: "..." },
//     { index: 8, score: 0.76, document: "..." }
//   ]
// }

// 3. Top 3'Ã¼ seÃ§
const bestDocuments = rerankerResult.ranked_documents
  .slice(0, 3)
  .map(r => topDocuments[r.index]);

// 4. LLM'e gÃ¶nder
const context = bestDocuments
  .map(d => d.content)
  .join("\n\n---\n\n");

const answer = await llm.invoke(`
  KAYNAKLAR:
  ${context}
  
  SORU: ${question}
  
  CEVAP:
`);
```

## **Ã–zel Parametreler**

```python
# Tokenizer parametreleri
inputs = tokenizer(
    pairs,
    padding=True,           # Pad to max length
    truncation=True,        # Truncate > max_length
    return_tensors="pt",    # PyTorch tensors
    max_length=512          # Max input length
)

# Model parametreleri
logits = model(**inputs).logits
# logits: (batch_size, num_labels)
# num_labels = 2 (relevant/not relevant)

scores = torch.softmax(logits, dim=-1)[:, 1]
# [:, 1] = "relevant" class'Ä±n probability'si
# Range: 0.0 - 1.0
```

## **Ne Ä°ÅŸe Yarar?**

1. **SÄ±ralama** - 10 dÃ¶kÃ¼manÄ± relevance'e gÃ¶re sÄ±ralar
2. **Kalite** - KÃ¶tÃ¼ dokumalarÄ±nÄ± filtrer
3. **Efficiency** - LLM'e sadece en iyi 3'Ã¼ gÃ¶nder
4. **Fallback** - Qwen down ise Cohere kullan

---

# ğŸ”— **BÃœTÃœN AKIÅ: MODELLER NASIL BÄ°RLÄ°KTE Ã‡ALIÅIR?**

## **Upload AkÄ±ÅŸÄ±**

```
1. User: PDF yÃ¼kle
   â†“
2. Upload Route
   â”œâ”€ PDFLoader â†’ Sayfalardaki metni Ã§Ä±kar
   â”œâ”€ VLM (Port 8001) â†’ Her sayfa: "Bu tablo mÄ±?"
   â”‚  â””â”€ Qwen3-VL-4B: GÃ¶rsel analiz â†’ "Evet, tablo"
   â”œâ”€ Chunking â†’ 1000 char parÃ§alara bÃ¶l
   â”œâ”€ Embeddings (OpenAI API) â†’ Her chunk'Ä± 1536-dim vektÃ¶re Ã§evir
   â”‚  â””â”€ text-embedding-3-small: "Veri tabanÄ±" â†’ [0.123, ...]
   â””â”€ Database â†’ PostgreSQL'e kaydet
      
SonuÃ§: documents tablosunda 1000+ satÄ±r
```

## **Query AkÄ±ÅŸÄ±**

```
1. User: "Tablo 1'deki veriler neler?"
   â†“
2. Query Route
   â”œâ”€ Embeddings (OpenAI) â†’ "Tablo 1'deki veriler neler?" â†’ [0.111, ...]
   â”‚  â””â”€ text-embedding-3-small: Soruyu vektÃ¶re Ã§evir
   â”‚
   â”œâ”€ Vector Search (PostgreSQL pgvector)
   â”‚  â””â”€ Benzer 10 chunk â†’ SELECT ... ORDER BY <-> LIMIT 10
   â”‚
   â”œâ”€ Reranker (Port 8000) â†’ Top 10'u sÄ±rala â†’ Top 3 seÃ§
   â”‚  â””â”€ Qwen3-Reranker-4B: Query vs Doc relevance â†’ scores
   â”‚
   â”œâ”€ LLM (OpenAI API) â†’ Cevap oluÅŸtur
   â”‚  â””â”€ gpt-4o-mini: "Tablo 1 ÅŸu verileri iÃ§eriyor..."
   â”‚
   â””â”€ Response â†’ User'a cevabÄ± ve kaynaklarÄ± gÃ¶nder
   
SonuÃ§: "Cevap + Kaynaklar"
```

---

# ğŸ“Š **MODELLER KARÅILAÅTIRMASI**

## **HÄ±z (CPU'da)**

```
OpenAI text-embedding-3-small:  ~50ms
Qwen3-Reranker-4B:              ~500ms (10 docs)
Qwen3-VL-4B-Instruct:           ~3-5s (sayfa baÅŸÄ±na)
OpenAI gpt-4o-mini:             ~2-5s (soruya gÃ¶re)

Total Query Time (Ã¶rnek):
- Embed soru:     50ms
- Vector search:  100ms
- Reranking:      500ms
- LLM:            3-5s
_______________
Total: ~3.7-5.7 saniye
```

## **Maliyet (aylÄ±k tahmini, 1000 query)**

```
OpenAI text-embedding-3-small:  ~ $1-2
OpenAI gpt-4o-mini:              ~ $10-20
Qwen3-Reranker-4B (local):       $0 (one-time 8GB disk)
Qwen3-VL-4B-Instruct (local):    $0 (one-time 8GB disk)
_______________
Total: ~$11-22/ay
```

## **Kalite (0-10 scale)**

```
Embedding (Relevance):           8/10  (text-embedding-3-small Ã§ok iyi)
LLM (Cevap Kalitesi):            9/10  (gpt-4o-mini Ã§ok gÃ¼Ã§lÃ¼)
Reranker (SÄ±ralama DoÄŸruluÄŸu):   8/10  (Qwen3-Reranker Ã§ok iyi)
VLM (GÃ¶rsel Anlama):             7/10  (Qwen3-VL CPU'da yavaÅŸ ama doÄŸru)
```

---

# ğŸ›ï¸ **HIPERPARAMETER AYARLAMA**

## **Temperature (Rasgelelik)**

```typescript
// Åu an: temperature = 0.1

// DeÄŸiÅŸtirmek istersen:
// 0.0 = Tamamen deterministik (test/production)
// 0.5 = Orta (balanced)
// 1.0 = YaratÄ±cÄ± (brainstorming)

// Kod:
export const llm = new ChatOpenAI({
  temperature: 0.1,  // â† BurasÄ±
});
```

## **Chunk Size (ParÃ§a BÃ¼yÃ¼klÃ¼ÄŸÃ¼)**

```typescript
// Åu an: 1000 char

const textSplitter = new RecursiveCharacterTextSplitter({
  chunkSize: 1000,      // â† BurasÄ±
  chunkOverlap: 200,
});

// DeÄŸiÅŸtirmek istersen:
// 500 = Daha kÃ¼Ã§Ã¼k chunk â†’ daha fazla, ama az context
// 2000 = Daha bÃ¼yÃ¼k chunk â†’ az chunk, daha fazla context
```

## **Top K (Vector Search)**

```typescript
// Åu an: LIMIT 10

const results = await pool.query(`
  SELECT * FROM documents
  ORDER BY embedding <-> $1::vector
  LIMIT 10  -- â† BurasÄ±
`);

// DeÄŸiÅŸtirmek istersen:
// 5 = Daha az â†’ hÄ±zlÄ± ama az dÃ¶kÃ¼man
// 20 = Daha fazla â†’ daha kapsamlÄ± ama yavaÅŸ
```

## **Reranker Top K**

```typescript
// Åu an: top_k = 3

const rerankerResult = await fetch('http://localhost:8000/rerank', {
  body: JSON.stringify({
    top_k: 3  // â† BurasÄ±
  })
});

// DeÄŸiÅŸtirmek istersen:
// 1 = Sadece en iyi 1
// 5 = Top 5
// (LLM'e gÃ¶nderilecek context miktarÄ±nÄ± etkiler)
```

---

# ğŸ› **SORUN Ã‡Ã–ZME**

## **"VLM yanÄ±t vermiyor"**

```bash
# 1. Check health
curl http://localhost:8001/health

# 2. Check logs
tail -100 vlm_server.log

# 3. Restart
pkill -f vlm_server
source vlm_env/bin/activate
python3 vlm_server.py &
```

## **"Reranker Ã§ok yavaÅŸ"**

```python
# Sebep: CPU'da Ã§alÄ±ÅŸÄ±yor
# Ã‡Ã¶zÃ¼m: GPU yap

# reranker_server.py
device = torch.device("cuda")  # CUDA enable et

# GPU var mÄ± check:
nvidia-smi
```

## **"Embedding kalitesi kÃ¶tÃ¼"**

```typescript
// Sebep: Model yanlÄ±ÅŸ seÃ§ilmiÅŸ
// Ã‡Ã¶zÃ¼m: Daha iyi model kullan

new OpenAIEmbeddings({
  modelName: "text-embedding-3-large"  // small â†’ large
  // ama 3x pahalÄ± ve yavaÅŸ
});
```

---

**SonuÃ§:** Bu 4 model birlikte Ã§alÄ±ÅŸarak eksiksiz bir RAG sistemi oluÅŸtururlar. Her modelin kendi iÅŸi, kendi parametreleri, kendi optimizasyonu var. ğŸš€
