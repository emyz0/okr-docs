#!/usr/bin/env python3
"""
ğŸ¤– QWEN3 RERANKER SERVER
FastAPI ile Qwen/Qwen3-Reranker-4B modelini Ã§alÄ±ÅŸtÄ±ran server
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List
import torch
import torch.nn.functional as F
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import logging

# Logging ayarla
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Qwen3 Reranker Server", version="1.0")

# Global model ve tokenizer (sunucu baÅŸlangÄ±cÄ±nda yÃ¼klenir)
model = None
tokenizer = None
device = None

class RerankerRequest(BaseModel):
    """Reranking isteÄŸi"""
    query: str
    documents: List[str]
    top_k: int = 10

class RerankerResponse(BaseModel):
    """Reranking yanÄ±tÄ±"""
    query: str
    ranked_documents: List[dict]  # [{"index": 0, "document": "...", "score": 0.95}, ...]
    total_documents: int

@app.on_event("startup")
async def load_model():
    """Sunucu baÅŸlatÄ±ldÄ±ÄŸÄ±nda model yÃ¼kle"""
    global model, tokenizer, device
    
    logger.info("ğŸ¤– Qwen3-Reranker-4B model yÃ¼kleniyor...")
    
    # Device seÃ§ (GPU varsa kullan)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ğŸ“ Device: {device}")
    
    # Model ve tokenizer yÃ¼kle
    model_name = "Qwen/Qwen3-Reranker-4B"
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        
        # Padding token'Ä±nÄ± ayarla - Qwen iÃ§in kritik!
        if tokenizer.pad_token is None:
            tokenizer.pad_token = "<|endoftext|>"  # Qwen pad token'Ä±
        
        logger.info(f"âœ… Tokenizer yÃ¼klendi (pad_token={tokenizer.pad_token}, pad_token_id={tokenizer.pad_token_id})")
        
        model = AutoModelForSequenceClassification.from_pretrained(
            model_name, 
            trust_remote_code=True,
            torch_dtype=torch.float32,
            pad_token_id=tokenizer.pad_token_id  # Kritik! Model'a pad_token_id'yi ver
        ).to(device)
        model.eval()  # Evaluation mode
        
        logger.info("âœ… Model baÅŸarÄ±yla yÃ¼klendi")
    except Exception as e:
        logger.error(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
        raise

@app.post("/rerank", response_model=RerankerResponse)
async def rerank(request: RerankerRequest) -> RerankerResponse:
    """
    Sorgu ve dokÃ¼manlara gÃ¶re rerank yapÄ±p en iyi sonuÃ§larÄ± dÃ¶ndÃ¼r
    
    Args:
        request.query: Arama sorgusu
        request.documents: DokÃ¼mantasyon listesi
        request.top_k: KaÃ§ tane dÃ¶ndÃ¼rÃ¼lecek
    
    Returns:
        RerankerResponse: SÄ±ralanmÄ±ÅŸ dokÃ¼mantasyon
    """
    if not model or not tokenizer:
        raise HTTPException(status_code=500, detail="Model yÃ¼klenmedi")
    
    if not request.documents:
        raise HTTPException(status_code=400, detail="DokÃ¼mantasyon boÅŸ")
    
    try:
        logger.info(f"ğŸ”„ Reranking baÅŸladÄ±: sorgu='{request.query[:50]}...', dokÃ¼={len(request.documents)}")
        
        # Her dokÃ¼mantÄ± sorgu ile pair yap
        pairs = [[request.query, doc] for doc in request.documents]
        
        # Tokenize et - batch processing optimize et
        with torch.no_grad():
            # Daha kÃ¼Ã§Ã¼k batch'ler ile iÅŸle (CPU'da daha hÄ±zlÄ±)
            batch_size = 4
            all_scores = []
            
            for i in range(0, len(pairs), batch_size):
                batch_pairs = pairs[i:i+batch_size]
                
                inputs = tokenizer(
                    batch_pairs,
                    padding="max_length",  # Explicit padding
                    truncation=True,
                    return_tensors='pt',
                    max_length=256  # CPU performansÄ± iÃ§in dÃ¼ÅŸÃ¼rdÃ¼k (512'den)
                ).to(device)
                
                logger.info(f"   ï¿½ Batch {i//batch_size + 1}: {len(batch_pairs)} pair iÅŸleniyor...")
                
                # Model Ã§alÄ±ÅŸtÄ±r
                try:
                    outputs = model(**inputs)
                    probs = F.softmax(outputs.logits, dim=-1)  # Softmax
                    batch_scores = probs[:, 1].cpu().tolist()  # Relevant class'Ä±n probability'si
                    all_scores.extend(batch_scores)
                    
                except Exception as e:
                    logger.error(f"   âŒ Batch hatasÄ±: {str(e)}")
                    # Fallback: bu batch iÃ§in dummy scores
                    all_scores.extend([0.5] * len(batch_pairs))
            
            scores = all_scores
            logger.info(f"   âœ… Scores hesaplandÄ±: {scores[:3]}...")
        
        # Skor ile indeks pair yap
        scored_docs = [
            {
                "index": idx,
                "document": doc,
                "score": float(score)
            }
            for idx, (doc, score) in enumerate(zip(request.documents, scores))
        ]
        
        # Skor'a gÃ¶re azalan sÄ±rada sÄ±rala
        ranked = sorted(scored_docs, key=lambda x: x["score"], reverse=True)
        
        # Top K al
        top_k = min(request.top_k, len(ranked))
        ranked = ranked[:top_k]
        
        logger.info(f"âœ… Reranking tamamlandÄ±: top {top_k} seÃ§ildi")
        logger.info(f"   En yÃ¼ksek skor: {ranked[0]['score']:.4f}")
        
        return RerankerResponse(
            query=request.query,
            ranked_documents=ranked,
            total_documents=len(request.documents)
        )
        
    except Exception as e:
        logger.error(f"âŒ Reranking hatasÄ±: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Reranking hatasÄ±: {str(e)}")

@app.get("/health")
async def health():
    """Sunucu saÄŸlÄ±k kontrolÃ¼"""
    return {
        "status": "healthy",
        "model": "Qwen/Qwen3-Reranker-4B",
        "device": str(device),
        "model_loaded": model is not None
    }

@app.get("/")
async def root():
    """Ana sayfa"""
    return {
        "name": "Qwen3 Reranker Server",
        "version": "1.0",
        "endpoints": [
            "/rerank (POST) - DokÃ¼mantasyonu sÄ±rala",
            "/health (GET) - Sunucu durumu"
        ]
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
