#!/bin/bash
# ğŸš€ VLM Server - Lokal Transformers ile BaÅŸlat
# Qwen2.5-VL-7B-Instruct (GPU/CPU)

set -e

echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "ğŸ–¼ï¸  VLM SERVER (LOKAL TRANSFORMERS)"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

# Ortam deÄŸiÅŸkenleri
export VLM_MODEL="Qwen/Qwen2.5-VL-7B-Instruct"

# .env.local yÃ¼kle (opsiyonel - lokal inference iÃ§in gerek yok)
if [ -f ".env.local" ]; then
    echo "ğŸ“ .env.local yÃ¼kleniyor..."
    export $(grep -v '^#' .env.local | xargs)
fi

# Virtual environment
if [ ! -d "vlm_env" ]; then
    echo "ğŸ”¨ Virtual environment oluÅŸturuluyor..."
    python3 -m venv vlm_env
fi

source vlm_env/bin/activate

# Dependencies
echo "ğŸ“¦ Dependencies kontrol..."
if ! python -c "import torch" 2>/dev/null; then
    echo "ğŸ“¥ Paketler yÃ¼kleniyor (ilk sefer uzun sÃ¼rebilir)..."
    pip install -q -r vlm_transformers_requirements.txt
fi

# Device kontrol
if python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')" | grep -q "CUDA: True"; then
    DEVICE="GPU (CUDA)"
else
    DEVICE="CPU"
fi

echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "ğŸš€ Server baÅŸlatÄ±lÄ±yor..."
echo "   Model: $VLM_MODEL"
echo "   Provider: Lokal Transformers"
echo "   Device: $DEVICE"
echo "   Port: 8001"
echo "   Health: http://localhost:8001/health"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

python vlm_transformers_server.py

