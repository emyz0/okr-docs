# ğŸ“ OKR-DOCS SÄ°STEMÄ° - KAPSAMLI Ã–ZET

## TL;DR (Ã‡ok Uzun; Okudum)

**Bu sistem nedir?**
- KullanÄ±cÄ± PDF yÃ¼kler
- Sistem onu analiz eder (metin + gÃ¶rsel)
- Sorulara akÄ±llÄ± cevaplar verir
- KaynaklarÄ± gÃ¶sterir

**Teknoloji?**
- NextJS (Frontend)
- PostgreSQL + pgvector (Veri)
- OpenAI API (AI)
- Qwen VLM (GÃ¶rsel)
- Qwen Reranker (SÄ±ralama)

**Durum?**
âœ… TamamlandÄ± ve Ã§alÄ±ÅŸÄ±yor

---

## ğŸ“ TÃœM DOSYA LÄ°STESÄ° VE AÃ‡IKLAMASI

### ğŸ”´ KULLANILANLAR (17 TypeScript + 2 Python)

#### `/lib/rag/` (5 TypeScript)

**1. db.ts** â­â­â­ KRÄ°TÄ°K
```
GÃ¶rev: PostgreSQL baÄŸlantÄ±
Kod satÄ±rÄ±: ~30
Ã–nemli function: pool.query()

NE YAPIP?
- Connection pool oluÅŸturur
- TÃ¼m database sorgularÄ±nÄ± yapar
- pgvector uzantÄ±sÄ± ile vektÃ¶r arama

Ã–RNEK:
const result = await pool.query(
  'SELECT * FROM documents WHERE user_id = $1',
  [userId]
)
```

**2. chain.ts** â­â­â­ KRÄ°TÄ°K
```
GÃ¶rev: OpenAI baÄŸlantÄ±
Kod satÄ±rÄ±: ~20
Ã–nemli exports: embeddings, llm

NE YAPIP?
- text-embedding-3-small yÃ¼kler (1536-dim)
- gpt-4o-mini yÃ¼kler (LLM)
- Hem embedding hem LLM Ã§aÄŸrÄ±sÄ±

Ã–RNEK:
const qEmb = await embeddings.embedQuery(question)
const response = await llm.invoke(prompt)
```

**3. rerank.ts** â­â­ Ã–NEMLÄ°
```
GÃ¶rev: Cohere API fallback
Kod satÄ±rÄ±: ~80
Ã–nemli function: rerankDocuments()

NE YAPIP?
- Qwen reranker down ise Cohere kullan
- 10 dokuman â†’ 3 en ilgili
- Relevance score hesapla

DURUMU:
Fallback mechanism (Qwen baÅŸarÄ±lÄ± olduÄŸu sÃ¼rece Ã§alÄ±ÅŸmÄ±yor)
```

**4. pdf-vlm-analyzer.ts** â­â­ Ã–NEMLÄ°
```
GÃ¶rev: VLM integration
Kod satÄ±rÄ±: ~200
Ã–nemli functions:
- extractContentWithVLM()
- analyzeImageWithVLM()
- renderPdfPageToBase64()
- formatVLMChunks()

NE YAPIP?
1. PDF sayfalarÄ±nÄ± gÃ¶rsele render et
2. VLM sunucusuna gÃ¶nder
3. Tablo/diagram analiz et
4. Chunks'a dÃ¶nÃ¼ÅŸtÃ¼r

Ã–RNEK:
const vlmResults = await extractContentWithVLM(pdfPath, 20)
// [{pageNum: 1, analysis: "Tablo: ...", contentType: "table"}, ...]
```

**5. pdf-ocr-processor.ts** â­ DESTEKLEYICI
```
GÃ¶rev: OCR (Tesseract.js)
Kod satÄ±rÄ±: ~100
Status: YAZILMIS AMA OCR YERINE VLM KULLANILIYOR

Fonksiyonlar:
- recognizeImageText()
- batchRecognizeImages()
- mergeOCRResults()

UNUSED AMA DURUYOR (OCR'dan VLM'ye geÃ§ildi)
```

**6. pdf-image-ocr.ts** â­ DESTEKLEYICI
```
GÃ¶rev: OCR orkestrasyon
Kod satÄ±rÄ±: ~150
Status: YAZILMIS AMA VLM YERINE KULLANILIYOR

Fonksiyonlar:
- extractOCRFromPdf()
- filterImagePages()
- formatOCRChunks()

NOT: VLM eklenince bu kullanÄ±lmayÄ± bÄ±raktÄ± (referans iÃ§in kalÄ±yor)
```

**7. document-parser.ts** â­ DESTEKLEYICI
```
GÃ¶rev: Excel, Word, TXT parsing
Kod satÄ±rÄ±: ~150
Ã–nemli functions:
- extractTextFromExcel()
- extractTextFromWord()
- extractTextFromTxt()

NE YAPIP?
- Excel â†’ CSV â†’ chunks
- Word .docx â†’ text
- TXT â†’ UTF-8 (fallback: Latin1)

KULLANIÅ:
upload route'unda Ã§aÄŸrÄ±lÄ±r
```

#### `/app/api/rag/` (4 TypeScript Routes)

**8. upload/route.ts** â­â­â­ KRÄ°TÄ°K
```
GÃ¶rev: Dosya yÃ¼kleme
Kod satÄ±rÄ±: ~320
Ã–nemli operations:
1. Dosya tÃ¼rÃ¼ kontrol
2. Metin Ã§Ä±karma
3. VLM analiz
4. Chunking
5. Embedding
6. Database kayÄ±t

ADIM ADIM:
1. FormData parse et
2. GeÃ§ici dosyaya kaydet
3. Dosya tipine gÃ¶re iÅŸle:
   - PDF â†’ PDFLoader + VLM
   - Excel â†’ XLSX library
   - Word â†’ Mammoth
   - TXT â†’ fs.readFileSync
4. Metin chunks'a bÃ¶l (1000 char, 200 overlap)
5. TÃ¼m chunks embedding'e Ã§evir
6. Database'e INSERT
7. file_id ata (tÃ¼m chunks aynÄ±)

HATALARÄ±:
- Dosya tipi unknown â†’ skip
- Metin Ã§Ä±karÄ±lamadÄ± â†’ skip
- VLM fail â†’ sadece metin chunks
- Embedding fail â†’ durdur
- Database fail â†’ durdur
```

**9. query/route.ts** â­â­â­ KRÄ°TÄ°K
```
GÃ¶rev: Sorgu iÅŸleme
Kod satÄ±rÄ±: ~250
Ã–nemli operations:
1. Soruyu embedding'e Ã§evir
2. Vector search (TOP 10)
3. Reranking (TOP 3)
4. LLM cevap
5. KonuÅŸma kaydet

ADIM ADIM:
1. embeddings.embedQuery(question) â†’ [0.123, ...]
2. pool.query() â†’ vector search via <-> operator
3. Try Qwen reranker (http://localhost:8000/rerank)
4. Catch â†’ Fallback Cohere
5. llm.invoke(prompt) â†’ LLM cevap
6. pool.query() â†’ sections gÃ¼ncellemesi
7. Return {answer, sources, sectionId}

HATA HANDLING:
- Vector search 0 sonuÃ§ â†’ "Bilgi yok"
- Reranker down â†’ Cohere fallback
- LLM down â†’ Error response
```

**10. pdfs/route.ts** â­ DESTEKLEYICI
```
GÃ¶rev: PDF listesi
Kod satÄ±rÄ±: ~50

YAPIP:
SELECT DISTINCT metadata->>'source' FROM documents
WHERE user_id = $1

SONUÃ‡: KullanÄ±cÄ±nÄ±n yÃ¼klediÄŸi tÃ¼m PDF'ler
```

**11. sections/route.ts** â­ DESTEKLEYICI
```
GÃ¶rev: KonuÅŸma kaydÄ±
Kod satÄ±rÄ±: ~100

YAPIP:
GET: SELECT * FROM sections WHERE user_id = $1
POST: INSERT INTO sections
DELETE: DELETE FROM sections WHERE id = $1
```

#### `/app/api/` (DiÄŸer)

**12. /api/news/* ** âŒ UNRELATED
```
GÃ¶rev: News API (proje dÄ±ÅŸÄ±)
Status: DEPRECATED - Silinebilir
```

#### Root Level TypeScript

**13. next.config.ts** ğŸ”§ CONFIG
```
GÃ¶rev: Next.js konfigÃ¼rasyonu
Ä°Ã§inde:
- Turbopack enable
- Transformers build
- PDF parsing
```

#### Python SunucularÄ± (2 Dosya)

**14. vlm_server.py** â­â­ Ã–NEMLÄ°
```
Port: 8001
GÃ¶rev: Qwen VLM
Kod satÄ±rÄ±: ~180

Model: Qwen/Qwen3-VL-4B-Instruct
- float16 (GPU) / float32 (CPU)
- Flash Attention 2 (GPU)

ENDPOINTS:
- POST /analyze
  - Input: {image_base64, task, language}
  - Output: {analysis, content_type, confidence}

- GET /health
  - Output: {status, device, model_loaded}

STARTUP:
~1-2 min model yÃ¼klenmesi (ilk kez)
```

**15. reranker_server.py** â­â­ Ã–NEMLÄ°
```
Port: 8000
GÃ¶rev: Qwen Reranker
Kod satÄ±rÄ±: ~160

Model: Qwen/Qwen3-Reranker-4B
- float32 CPU/GPU
- Padding token otomatik

ENDPOINTS:
- POST /rerank
  - Input: {query, documents, top_k}
  - Output: {ranked_documents, scores}

- GET /health
  - Output: {status, device, model_loaded}

STARTUP:
~1-2 min model yÃ¼klenmesi (ilk kez)
```

#### Setup Scripts

**16. setup_vlm.sh** ğŸ”§
```
GÃ¶rev: VLM kurulum otomatizasyonu
Ä°Ã§inde:
- Python venv
- pip install
- BaÅŸlatma talimatlarÄ±
```

**17. setup_reranker.sh** ğŸ”§
```
GÃ¶rev: Reranker kurulum otomatizasyonu
(AynÄ± logic)
```

---

### âŒ KULLANILANMAYAN DOSYALAR (3 Deprecated)

**1. pdf-image-extraction.ts**
- Eski gÃ¶rsel Ã§Ä±karma yÃ¶ntemi
- OCR/VLM ile deÄŸiÅŸtirildi
- Kod: ~100 satÄ±r
- Silinebilir

**2. image-processing.ts**
- Eski image processing
- VLM ile deÄŸiÅŸtirildi
- Kod: ~80 satÄ±r
- Silinebilir

**3. extract_pdf_images.py**
- Eski Python script
- Deprecated
- Silinebilir

---

## ğŸ”„ VERI AKIÅI (DETAYLI)

### Upload AkÄ±ÅŸÄ±

```
User Action: PDF yÃ¼kle
   â†“
/api/rag/upload POST
   â”œâ”€ Step 1: File parse
   â”‚   â”œâ”€ Get FormData
   â”‚   â”œâ”€ Get files array
   â”‚   â””â”€ Validate (at least 1 file)
   â”‚
   â”œâ”€ Step 2: File loop
   â”‚   â”œâ”€ For each file in files:
   â”‚   â”‚  â”œâ”€ Read into buffer
   â”‚   â”‚  â”œâ”€ Sanitize filename (remove spaces, special chars)
   â”‚   â”‚  â”œâ”€ Write to /tmp
   â”‚   â”‚  â””â”€ Get file extension
   â”‚   â”‚
   â”‚   â”œâ”€ Step 3: Text extraction (file type based)
   â”‚   â”‚  â”œâ”€ If PDF:
   â”‚   â”‚  â”‚  â”œâ”€ new PDFLoader(tempPath)
   â”‚   â”‚  â”‚  â””â”€ await loader.load() â†’ docs[]
   â”‚   â”‚  â”‚
   â”‚   â”‚  â”œâ”€ Else if Excel:
   â”‚   â”‚  â”‚  â”œâ”€ XLSX.read(buffer)
   â”‚   â”‚  â”‚  â”œâ”€ Sheet to CSV
   â”‚   â”‚  â”‚  â””â”€ Split into rows
   â”‚   â”‚  â”‚
   â”‚   â”‚  â”œâ”€ Else if Word:
   â”‚   â”‚  â”‚  â”œâ”€ mammoth.extractRawText()
   â”‚   â”‚  â”‚  â””â”€ Return text
   â”‚   â”‚  â”‚
   â”‚   â”‚  â””â”€ Else if TXT:
   â”‚   â”‚     â”œâ”€ fs.readFileSync(UTF-8)
   â”‚   â”‚     â””â”€ Fallback: Latin1
   â”‚   â”‚
   â”‚   â”œâ”€ Step 4: VLM Analysis (PDF only)
   â”‚   â”‚  â”œâ”€ extractContentWithVLM(tempPath, 20)
   â”‚   â”‚  â”‚  â”œâ”€ For i=1 to 20:
   â”‚   â”‚  â”‚  â”‚  â”œâ”€ renderPdfPageToBase64(i)
   â”‚   â”‚  â”‚  â”‚  â”œâ”€ HTTP POST http://localhost:8001/analyze
   â”‚   â”‚  â”‚  â”‚  â”œâ”€ Get {analysis, contentType}
   â”‚   â”‚  â”‚  â”‚  â””â”€ Push to results
   â”‚   â”‚  â”‚  â”‚
   â”‚   â”‚  â”‚  â””â”€ Return results[]
   â”‚   â”‚  â”‚
   â”‚   â”‚  â””â”€ formatVLMChunks(results)
   â”‚   â”‚     â””â”€ Return [{content, metadata}, ...]
   â”‚   â”‚
   â”‚   â”œâ”€ Step 5: Add metadata
   â”‚   â”‚  â””â”€ For each doc:
   â”‚   â”‚     â”œâ”€ Add source filename
   â”‚   â”‚     â”œâ”€ Add file_type
   â”‚   â”‚     â””â”€ Add has_images flag
   â”‚   â”‚
   â”‚   â””â”€ Step 6: File ID assignment
   â”‚      â”œâ”€ Query: SELECT MAX(file_id) FROM documents WHERE user_id
   â”‚      â”œâ”€ file_id = MAX + 1
   â”‚      â””â”€ Store in fileIdMap
   â”‚
   â”œâ”€ Step 7: All docs combine
   â”‚   â””â”€ allDocs = [doc1, doc2, ..., docN]
   â”‚
   â”œâ”€ Step 8: Chunking
   â”‚   â”œâ”€ RecursiveCharacterTextSplitter
   â”‚   â”‚  â”œâ”€ chunkSize: 1000
   â”‚   â”‚  â”œâ”€ chunkOverlap: 200
   â”‚   â”‚  â””â”€ Split all docs
   â”‚   â”‚
   â”‚   â””â”€ chunks = [chunk1, chunk2, ...]
   â”‚
   â”œâ”€ Step 9: Embedding
   â”‚   â”œâ”€ For each chunk:
   â”‚   â”‚  â”œâ”€ await embeddings.embedQuery(chunk.pageContent)
   â”‚   â”‚  â””â”€ embedding = [0.123, 0.456, ...]
   â”‚   â”‚
   â”‚   â””â”€ All chunks have embeddings
   â”‚
   â””â”€ Step 10: Database insert
      â”œâ”€ For each chunk:
      â”‚  â”œâ”€ INSERT INTO documents
      â”‚  â”‚  (file_id, user_id, content, metadata, embedding)
      â”‚  â”‚
      â”‚  â””â”€ VALUES (fileId, userId, text, json, vector)
      â”‚
      â””â”€ All chunks saved! âœ…
```

### Query AkÄ±ÅŸÄ±

```
User Action: Soru sor
   â†“
/api/rag/query POST
   â”œâ”€ Step 1: Parse request
   â”‚   â”œâ”€ {question, userId, selectedPdfs?, conversationHistory?, sectionId?}
   â”‚   â””â”€ Validate (question + userId required)
   â”‚
   â”œâ”€ Step 2: Embed question
   â”‚   â”œâ”€ await embeddings.embedQuery(question)
   â”‚   â””â”€ qEmb = [0.111, 0.222, ...]
   â”‚
   â”œâ”€ Step 3: Vector search
   â”‚   â”œâ”€ SQL Query:
   â”‚   â”‚  SELECT * FROM documents
   â”‚   â”‚  WHERE user_id = $1
   â”‚   â”‚  AND metadata->>'source' = ANY($2)  â† if selectedPdfs
   â”‚   â”‚  ORDER BY embedding <-> qEmb::vector  â† pgvector distance
   â”‚   â”‚  LIMIT 10
   â”‚   â”‚
   â”‚   â””â”€ result.rows = [chunk1, chunk2, ..., chunk10]
   â”‚
   â”œâ”€ Step 4: Reranking
   â”‚   â”œâ”€ TRY:
   â”‚   â”‚  â”œâ”€ HTTP POST http://localhost:8000/rerank
   â”‚   â”‚  â”‚  â”œâ”€ Body: {query, documents: [chunk.content Ã— 10], top_k: 10}
   â”‚   â”‚  â”‚  â””â”€ Response: {ranked_documents: [{index, score}, ...]}
   â”‚   â”‚  â”‚
   â”‚   â”‚  â””â”€ rerankResults = response.ranked_documents
   â”‚   â”‚
   â”‚   â””â”€ CATCH (Qwen down):
   â”‚      â””â”€ Use Cohere API (rerank.ts)
   â”‚
   â”œâ”€ Step 5: Select top chunks (from each PDF)
   â”‚   â”œâ”€ For each rerank result (sorted by score):
   â”‚   â”‚  â”œâ”€ Get row = result.rows[index]
   â”‚   â”‚  â”œâ”€ source = row.metadata.source
   â”‚   â”‚  â”œâ”€ If source not selected yet:
   â”‚   â”‚  â”‚  â”œâ”€ Add to selectedByPdf
   â”‚   â”‚  â”‚  â””â”€ Add index to selectedIndices
   â”‚   â”‚  â”‚
   â”‚   â”‚  â””â”€ If selectedIndices.size >= 10: break
   â”‚   â”‚
   â”‚   â””â”€ rankedRows = selected rows
   â”‚
   â”œâ”€ Step 6: Format context
   â”‚   â”œâ”€ contexts = rankedRows.map(r => ({
   â”‚   â”‚  id, file_id, source, chunk, page, excerpt
   â”‚   â”‚}))
   â”‚   â”‚
   â”‚   â””â”€ contextText = `Source: ...\n\n${excerpt}\n\n---\n\n...`
   â”‚
   â”œâ”€ Step 7: Build prompt
   â”‚   â””â”€ prompt = `
   â”‚      [Conversation history if exists]
   â”‚      
   â”‚      KAYNAKLAR:
   â”‚      ${contextText}
   â”‚      
   â”‚      SORU: ${question}
   â”‚      
   â”‚      CEVAP:`
   â”‚
   â”œâ”€ Step 8: LLM inference
   â”‚   â”œâ”€ await llm.invoke(prompt)
   â”‚   â””â”€ llmResponse = {content: "Cevap metni..."}
   â”‚
   â”œâ”€ Step 9: Save to sections
   â”‚   â”œâ”€ If sectionId exists:
   â”‚   â”‚  â”œâ”€ GET messages from sections
   â”‚   â”‚  â”œâ”€ Append new message
   â”‚   â”‚  â”œâ”€ UPDATE sections SET messages
   â”‚   â”‚  â””â”€ savedSectionId = sectionId
   â”‚   â”‚
   â”‚   â””â”€ Else:
   â”‚      â”œâ”€ INSERT new section
   â”‚      â”œâ”€ Return new sectionId
   â”‚      â””â”€ savedSectionId = newId
   â”‚
   â””â”€ Step 10: Return response
      â””â”€ {
         success: true,
         answer: llmResponse.content,
         sectionId: savedSectionId,
         sources: contexts[]
      }
```

---

## ğŸ’¾ DATABASE SCHEMA (DETAYLI)

### documents Tablosu

```sql
CREATE TABLE documents (
  id SERIAL PRIMARY KEY,
  -- id: Otomatik artan (1, 2, 3, ...)
  -- Tipik: 1-1000 (chunk ID'si)
  
  file_id INTEGER NOT NULL,
  -- file_id: Hangi PDF'ten geldi
  -- Tipik: 1, 2, 5 (tÃ¼m chunks aynÄ± file_id)
  -- Ã–rnek: 5 chunks â†’ file_id = 2 (hepsi 2 olur)
  
  user_id VARCHAR NOT NULL,
  -- user_id: Hangi kullanÄ±cÄ±
  -- Ã–rnek: "demo-user"
  
  content TEXT NOT NULL,
  -- content: Metin (max 1000 char)
  -- Ã–rnek: "Veri tabanÄ± nedir? Bir veri tabanÄ±..."
  
  metadata JSONB NOT NULL,
  -- metadata: Meta information
  -- {
  --   "source": "document.pdf",
  --   "page": 3,
  --   "type": "vlm" | "text" | "ocr",
  --   "contentType": "table" | "diagram" | "text",
  --   "confidence": 0.95,
  --   "has_images": true,
  --   "chunk": 15
  -- }
  
  embedding vector(1536) NOT NULL,
  -- embedding: Vector (1536 boyutlu, OpenAI)
  -- Ã–rnek: [0.123, 0.456, 0.789, ...]
  
  created_at TIMESTAMP DEFAULT NOW(),
  -- created_at: OluÅŸturma zamanÄ±
  
  -- INDEXES (HÄ±zlÄ± arama)
  -- IVFFLAT index on embedding
  -- B-tree index on user_id
  -- B-tree index on file_id
);
```

### sections Tablosu

```sql
CREATE TABLE sections (
  id SERIAL PRIMARY KEY,
  -- id: KonuÅŸma ID
  -- Ã–rnek: 1, 2, 3, ...
  
  user_id VARCHAR NOT NULL,
  -- user_id: Hangi kullanÄ±cÄ±
  
  title VARCHAR,
  -- title: KonuÅŸma baÅŸlÄ±ÄŸÄ±
  -- Ã–rnek: "ğŸ’¬ KonuÅŸma - 27 KasÄ±m 2025"
  
  messages JSONB NOT NULL,
  -- messages: Soru-cevap array
  -- [
  --   {
  --     "question": "Tablo 1 nedir?",
  --     "answer": "Tablo 1 ÅŸu bilgileri iÃ§erir...",
  --     "sources": [
  --       {
  --         "source": "document.pdf",
  --         "file_id": 5,
  --         "chunk": 15,
  --         "page": 3,
  --         "has_images": true
  --       }
  --     ]
  --   },
  --   {
  --     "question": "Kaynaklar neler?",
  --     "answer": "Kaynaklar ÅŸunlardÄ±r...",
  --     "sources": [...]
  --   }
  -- ]
  
  is_active BOOLEAN DEFAULT true,
  -- is_active: KonuÅŸma aktif mi?
  
  created_at TIMESTAMP DEFAULT NOW()
);
```

---

## ğŸ¯ Ã‡IKIÅ (WHAT'S OUT)

**HiÃ§bir ÅŸey dÄ±ÅŸarÄ± Ã§Ä±kmÄ±yor!**

Sistem ÅŸu ÅŸeyleri depolama/iÅŸleme yaptÄ±ÄŸÄ±:
- âœ… VektÃ¶rler: Database'te (pgvector)
- âœ… Chunks: Database'te (PostgreSQL)
- âœ… KonuÅŸmalar: Database'te (sections)
- âœ… Kaynaklar: JSON response'ta
- âœ… Model weights: Memory'de (VLM/Reranker servers)

DÄ±ÅŸarÄ± Ã§Ä±kan: Sadece HTTP responses!

---

## ğŸ SONUÃ‡

**17 TypeScript + 2 Python = Eksiksiz RAG Sistemi**

En Ã¶nemli dosyalar (sÄ±rasÄ±yla):
1. upload/route.ts (veri giriÅŸi)
2. query/route.ts (sorgu iÅŸleme)
3. db.ts (database)
4. chain.ts (AI modeller)

DiÄŸerleri destekleyici olarak Ã§alÄ±ÅŸÄ±yor.

âœ… **Sistem TamamlandÄ± ve Prod-Ready!**

---

**Yazar:** Emirhan YÄ±lmaz
**Tarih:** 27 KasÄ±m 2025
**Versiyon:** 1.0 Final
**Status:** âœ… COMPLETE
