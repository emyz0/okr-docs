# âœ… HUGGING FACE INFERENCE - KURULUM Ã–ZET

## ğŸ¯ Ne DeÄŸiÅŸti?

| Ã–nceki | Åimdi |
|--------|-------|
| Lokal VLM (4B parametre) | **Hugging Face Inference API (32B/14B)** |
| Lokal GPU/CPU gerekli | **Ä°nternet baÄŸlantÄ±sÄ± gerekli** |
| YavaÅŸ (CPU'da) | **HÄ±zlÄ± (cloud'da)** |
| 4 Milyar parametre | **32 Milyar parametre (daha gÃ¼Ã§lÃ¼)** |
| Tesseract OCR | **YalnÄ±zca VLM** |

---

## ğŸ“ YAPMAMAN GEREKENLER

**âŒ YAPMAYACAÄIZ:**
- Lokal olarak 32B model indirmek (200+ GB disk)
- GPU satÄ±n almak
- Model training yapÄ±lmayacak

**âœ… YAPACAÄIZ:**
- HuggingFace API token'Ä± almak (3 dakika)
- `.env.local` dosyasÄ±na token'Ä± eklemek
- `pip install httpx huggingface-hub` (1 dakika)
- Yeni `vlm_server.py` baÅŸlatmak

---

## ğŸš€ HIZLI BAÅLANGAÃ‡ (5 DAKIKA)

### AdÄ±m 1: Hugging Face Token OluÅŸtur
```
URL: https://huggingface.co/settings/tokens
âœ… "New token" â†’ Name: "okr-docs" â†’ Type: "Read" â†’ Create
ğŸ“‹ Token'Ä± kopyala: hf_aBcDeFg...
```

### AdÄ±m 2: .env.local'a Ekle
```bash
# Dosya: /Users/emirhanyilmaz/Desktop/okr-docs/.env.local

HUGGINGFACE_API_KEY=hf_YOUR_TOKEN_HERE
```

### AdÄ±m 3: Dependencies YÃ¼kle
```bash
source vlm_env/bin/activate
pip install -r vlm_requirements.txt
```

### AdÄ±m 4: VLM Sunucusunu BaÅŸlat
```bash
cd /Users/emirhanyilmaz/Desktop/okr-docs
source vlm_env/bin/activate
python3 vlm_server.py &
```

### AdÄ±m 5: Kontrol Et
```bash
curl http://localhost:8001/health
```

**Beklenen Ã§Ä±ktÄ±:**
```json
{
  "status": "healthy",
  "model": "Qwen/Qwen2-VL-32B-Instruct",
  "type": "hugging_face_inference",
  "api_key_set": true
}
```

---

## ğŸ”§ MODEL SEÃ‡Ä°MÄ°

Dosya: `vlm_server.py` â†’ SatÄ±r 35-39

```python
ACTIVE_MODEL = "32b"  # â† Buraya karar ver

# "32b" = Daha gÃ¼Ã§lÃ¼, biraz yavaÅŸ (10-15s/page)
# "14b" = Yeterli, hÄ±zlÄ± (5-8s/page)
```

---

## ğŸ’» DOSYA DURUMU

âœ… **YENI DOSYALAR:**
- `vlm_server.py` - Hugging Face Inference kullanÄ±yor (yeni)
- `HUGGINGFACE_SETUP.md` - Kurulum rehberi (detaylÄ±)

ğŸ“¦ **BACKUP:**
- `vlm_server_local.py.bak` - Eski lokal VLM (gerekirse restore et)

ğŸ—‘ï¸ **KALDIRILAN:**
- Tesseract.js dependency (package.json'dan)
- torch, transformers, opencv (vlm_requirements.txt'ten)

âœ… **DEÄÄ°ÅMEDÄ°:**
- Upload route (`app/api/rag/upload/route.ts`)
- Query route (`app/api/rag/query/route.ts`)
- Reranker server (`reranker_server.py`)
- Database (`lib/rag/db.ts`)

---

## ğŸ§ª TEST ADIMLARI

```bash
# 1. Terminal 1: Log'u izle
tail -f vlm_server.log

# 2. Terminal 2: Server'Ä± baÅŸlat
python3 vlm_server.py

# 3. Terminal 3: Health check
curl http://localhost:8001/health | python3 -m json.tool

# 4. Frontend'de PDF yÃ¼kle
# http://localhost:3001 â†’ Upload PDF â†’ Kontrol et

# 5. Logs'da "HF API'ye istek gÃ¶nderiliyor..." gÃ¶receksin
```

---

## ğŸ’° MALIYT HEÃœ

**Free Tier:** ~30,000 requests/ay
**Paid:** ~$0.000001-0.000003 per token

Ã–rnek: 100 sayfalÄ±k PDF = ~$0.001 (Ã§ok ucuz)

---

## âš ï¸ SORUN Ã‡Ã–ZME

### Hata: `API key not set`
```bash
# .env.local kontrol et
cat .env.local | grep HUGGINGFACE_API_KEY
```

### Hata: `504 Gateway Timeout`
- Model ilk kez loading (1-2 dakika bekle)
- Veya 14B model'e geÃ§ (daha hÄ±zlÄ±)

### Hata: `Connection refused`
- VLM server baÅŸlamadÄ±
- `python3 vlm_server.py` baÅŸlatmayÄ± dene

---

## ğŸ“š Ä°LGÄ°LÄ° DÃ–KÃœMANLAR

- `HUGGINGFACE_SETUP.md` - DetaylÄ± kurulum (oku!)
- `MODELS_EXPLAINED.md` - TÃ¼m modeller aÃ§Ä±klandÄ±
- `QUICK_REFERENCE.md` - HÄ±zlÄ± referans

---

## âœ… SÃœREÃ‡TÄ° Ã–ZETÄ°

1. âœ… VLM sunucusu HF Inference'e geÃ§irildi
2. âœ… GÃ¼Ã§lÃ¼ model (32B) tercih edildi
3. âœ… Requirements gÃ¼ncellendi (httpx, huggingface-hub)
4. âœ… Python syntax kontrol edildi
5. â³ **ADIM: Token oluÅŸturup .env.local'a ekle**
6. â³ **ADIM: Server baÅŸlat ve test et**

---

**SONRAKI ADIM:** Hugging Face hesabÄ±ndan token oluÅŸtur ve kurulum rehberini takip et! ğŸš€
