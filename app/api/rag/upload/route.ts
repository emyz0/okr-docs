// Next.js API Route: /api/rag/upload endpoint'i
// POST isteÄŸini handle eder ve FARKLI DOSYA TIPLERINI iÅŸler
// Desteklenen formatlar: PDF, Excel (.xlsx, .xls), Word (.docx), Text (.txt)

// ğŸ”§ DOMMatrix polyfill (pdfjs-dist + canvas iÃ§in Node.js ortamÄ±nda gerekli)
if (typeof global !== 'undefined' && !(global as any).DOMMatrix) {
  (global as any).DOMMatrix = class DOMMatrix {
    // Temel properties
    a: number = 1; 
    b: number = 0; 
    c: number = 0; 
    d: number = 1; 
    e: number = 0; 
    f: number = 0;
    
    // SVG transform matrix properties
    m11 = 1; m12 = 0; m13 = 0; m14 = 0;
    m21 = 0; m22 = 1; m23 = 0; m24 = 0;
    m31 = 0; m32 = 0; m33 = 1; m34 = 0;
    m41 = 0; m42 = 0; m43 = 0; m44 = 1;
    
    // Matrix flags
    is2D = true;
    isIdentity = true;
    
    constructor(values?: any) {
      if (values) {
        Object.assign(this, values);
      }
    }
    
    // Matrix operations (pdfjs compat)
    multiply(other: any) { return this; }
    inverse() { return this; }
    translate(x: number, y: number) { return this; }
    scale(x: number, y: number) { return this; }
    rotate(angle: number) { return this; }
    skewX(angle: number) { return this; }
    skewY(angle: number) { return this; }
    flipX() { return this; }
    flipY() { return this; }
  };
}

import { NextRequest, NextResponse } from 'next/server'
import { PDFLoader } from '@langchain/community/document_loaders/fs/pdf'
import { RecursiveCharacterTextSplitter } from '@langchain/textsplitters'
import { OpenAIEmbeddings } from "@langchain/openai"
import { pool } from '@/lib/rag/db'
import fs from 'fs'
import path from 'path'
import { extractTextFromExcel, extractTextFromWord, extractTextFromTxt } from '@/lib/rag/document-parser'

export async function POST(req: NextRequest) {
  // FormData'yÄ± parse et (dosyalarÄ± ve userId'yi oku)
  const formData = await req.formData()
  // formData.getAll('files'): SeÃ§ilen tÃ¼m PDF dosyalarÄ±nÄ± array olarak dÃ¶ndÃ¼rÃ¼r
  const files = formData.getAll('files') as File[]
  // userId: Hangi kullanÄ±cÄ± iÃ§in dosya yÃ¼kleniyorsa onun ID'si
  const userId = formData.get('userId')?.toString() || 'demo-user'

  // Validasyon: En az 1 PDF dosyasÄ± seÃ§ilmiÅŸ mi?
  if (!files || files.length === 0) {
    return NextResponse.json({ error: 'PDF bulunamadÄ±' }, { status: 400 })
  }

  console.log('Upload baÅŸladÄ±:', { userId, fileCount: files.length })

  try {
    // OpenAI embedding modeli baÅŸlat
    // text-embedding-3-small: KÃ¼Ã§Ã¼k ve hÄ±zlÄ± model (1536 boyutlu vektÃ¶r Ã¼retir)
    const embeddings = new OpenAIEmbeddings({
      apiKey: process.env.OPENAI_API_KEY!,
      modelName: "text-embedding-3-small",
    })

    // Her dosya iÃ§in ayrÄ± bir veri array'i tutuyoruz (chunk'larÄ± dosya baÅŸÄ±na sayabilmek iÃ§in)
    const allDocs: Array<{file: string, doc: any}> = []

    // âœ… HER DOSYA TIPINI Ä°ÅLE (PDF, Excel, Word, TXT)
    for (const file of files) {
      try {
        // DosyayÄ± buffer'a dÃ¶nÃ¼ÅŸtÃ¼r
        const arrayBuffer = await file.arrayBuffer()
        const buffer = Buffer.from(arrayBuffer)

        // GeÃ§ici dosya oluÅŸtur /tmp klasÃ¶rÃ¼nde
        // Dosya adÄ±nda boÅŸluk ve special karakterleri underscore ile deÄŸiÅŸtir
        const sanitizedName = file.name
          .replace(/\s+/g, '_')  // BoÅŸluklarÄ± underscore yap
          .replace(/[^\w.-]/g, '_')  // Word karakterleri, nokta, tire hariÃ§ hepsini underscore yap
        const tempPath = path.join('/tmp', sanitizedName)
        console.log(`ğŸ“ Temp dosya: ${tempPath}`)
        fs.writeFileSync(tempPath, buffer)
        
        // DosyanÄ±n yazÄ±ldÄ±ÄŸÄ±nÄ± kontrol et
        if (!fs.existsSync(tempPath)) {
          console.error(`âŒ Dosya yazÄ±lamadÄ±: ${tempPath}`)
          continue
        }
        console.log(`âœ… Dosya yazÄ±ldÄ±: ${fs.statSync(tempPath).size} byte`)

        const ext = path.extname(file.name).toLowerCase()
        let docs: any[] = []

        // ğŸ“„ DOSYA TIPINE GÃ–RE Ä°ÅLE
        if (ext === '.pdf') {
          // PDF iÅŸleme
          const loader = new PDFLoader(tempPath)
          docs = await loader.load()
          console.log(`ğŸ“‘ PDF: ${file.name} - ${docs.length} sayfa`)
          
          // ğŸ–¼ï¸ VLM ile gÃ¶rselleri ve tablolarÄ± analiz et (ZORUNLU - hem tablo hem grafik)
          console.log(`ğŸ” VLM analizi baÅŸlanÄ±yor...`)
          try {
            // VLM server'Ä± check et
            const healthCheck = await fetch('http://localhost:8001/health')
              .then(r => r.ok ? true : false)
              .catch(() => false)
            
            if (!healthCheck) {
              throw new Error('VLM server 8001 portunda eriÅŸilemez')
            }
            
            console.log(`âœ… VLM server saÄŸlÄ±klÄ±, analiz ediliyor...`)
            // VLM sunucusuna PDF sayfalarÄ±nÄ± gÃ¶nder ve tablo/grafikleri Ã§Ä±kart
            const vlmResults: any[] = []
            
            // Her PDF sayfasÄ± iÃ§in VLM'e sor
            for (let pageIdx = 0; pageIdx < docs.length; pageIdx++) {
              const page = docs[pageIdx];
              try {
                console.log(`  ğŸ“„ Sayfa ${pageIdx + 1}/${docs.length} analiz ediliyor...`)
                // VLM'e gÃ¶nder (sadece text-based analiz, gÃ¶rsel parsing PDFLoader'dan)
                const vlmResponse = await fetch('http://localhost:8001/analyze', {
                  method: 'POST',
                  headers: { 'Content-Type': 'application/json' },
                  body: JSON.stringify({
                    page_content: page.pageContent,
                    page_number: pageIdx + 1,
                    file_name: file.name
                  }),
                  signal: AbortSignal.timeout(5000) // 5 saniye timeout
                }).catch(() => null)
                
                if (vlmResponse && vlmResponse.ok) {
                  const vlmData = await vlmResponse.json()
                  if (vlmData.tables && vlmData.tables.length > 0) {
                    vlmResults.push({
                      page: pageIdx + 1,
                      tables: vlmData.tables,
                      has_analysis: true
                    })
                    console.log(`    âœ… ${vlmData.tables.length} tablo bulundu`)
                  }
                }
              } catch (pageError) {
                console.warn(`  âš ï¸ Sayfa ${pageIdx + 1} VLM analizi atlandÄ±`)
              }
            }
            
            if (vlmResults.length === 0) {
              console.log(`â„¹ï¸ VLM: Tablo analizi yapÄ±lmadÄ± (belgede tablo yok veya VLM analiz etmedi)`)
            } else {
              console.log(`âœ… VLM: ${vlmResults.length} sayfada tablo/grafik analizi yapÄ±ldÄ±`)
              
              // VLM sonuÃ§larÄ±nÄ± dokÃ¼manlara ekle
              const { formatVLMChunks } = await import('@/lib/rag/pdf-vlm-analyzer')
              const vlmChunks = await formatVLMChunks(vlmResults, file.name)
              vlmChunks.forEach((chunk) => {
                docs.push({
                  pageContent: chunk.content,
                  metadata: chunk.metadata
                })
              })
              
              console.log(`âœ… VLM chunks eklendi: toplam ${docs.length} dokuman`)
            }
          } catch (vlmError) {
            console.warn(`âš ï¸ VLM analizi atlandÄ±:`, vlmError instanceof Error ? vlmError.message : String(vlmError))
            // VLM hatasÄ± upload'Ä± durdurmaz, devam et
          }
        } 
        else if (ext === '.xlsx' || ext === '.xls') {
          // Excel iÅŸleme
          const excelText = await extractTextFromExcel(tempPath)
          if (excelText) {
            docs = [{
              pageContent: excelText,
              metadata: { 
                source: file.name, 
                type: 'excel',     // âœ… Excel chunks iÃ§in "excel"
                file_type: ext 
              }
            }]
            console.log(`ğŸ“Š Excel: ${file.name} - 1 dokuman (${excelText.length} karakter)`)
          }
        }
        else if (ext === '.docx') {
          // Word iÅŸleme
          const wordText = await extractTextFromWord(tempPath)
          if (wordText) {
            docs = [{
              pageContent: wordText,
              metadata: { 
                source: file.name, 
                type: 'word',      // âœ… Word chunks iÃ§in "word"
                file_type: ext 
              }
            }]
            console.log(`ğŸ“ Word: ${file.name} - 1 dokuman`)
          }
        } 
        else if (ext === '.txt') {
          // Text iÅŸleme
          const txtText = await extractTextFromTxt(tempPath)
          if (txtText) {
            docs = [{
              pageContent: txtText,
              metadata: { 
                source: file.name, 
                type: 'text',      // âœ… Text chunks iÃ§in "text"
                file_type: ext 
              }
            }]
            console.log(`ğŸ“„ Text: ${file.name} - 1 dokuman`)
          }
        }

        if (docs.length === 0) {
          console.warn(`âš ï¸  ${file.name}: Dokuman Ã§Ä±karÄ±lamadÄ±`)
          continue
        }

        // Excel dosyalarÄ± iÃ§in satÄ±rlarÄ± ayÄ±r (CSV satÄ±rlarÄ±nÄ± dokÃ¼manlara bÃ¶l)
        if (ext === '.xlsx' || ext === '.xls') {
          // CSV satÄ±rlarÄ±nÄ± dokÃ¼manlara dÃ¶nÃ¼ÅŸtÃ¼r
          const lines = docs[0].pageContent.split('\n').filter((line: string) => line.trim())
          const splitExcelDocs = lines.map((line: string, idx: number) => ({
            pageContent: line,
            metadata: {
              source: file.name,
              file_type: ext,
              page_index: idx + 1,
              has_images: false
            }
          }))
          allDocs.push(...splitExcelDocs.map((doc: any) => ({ file: file.name, doc })))
          continue
        }

        // ğŸ“‹ METADATA'YA KAYNAK BÄ°LGÄ°SÄ° EKLE (PDF, Word, TXT iÃ§in)
        const docsWithSource = docs.map((d, pageIdx) => {
          const meta = (d.metadata as any) || {}
          
          // GÃ¶rselleri algÄ±la
          const hasImages = d.pageContent.includes('[Image]') || 
                           d.pageContent.includes('table') ||
                           d.pageContent.includes('Tablo')
          
          return {
            ...d,
            metadata: { 
              ...meta, 
              source: file.name,
              type: 'pdf',         // âœ… TÃ¼r: PDF chunks
              file_type: ext,      // Dosya tipi
              has_images: hasImages,
              page_index: pageIdx + 1
            },
          }
        })
        
        allDocs.push(...docsWithSource.map(doc => ({ file: file.name, doc })))

        // GeÃ§ici dosyayÄ± sil
        fs.unlinkSync(tempPath)
      } catch (fileError: any) {
        console.error(`âŒ ${file.name} iÅŸleme hatasÄ±: ${fileError.message}`)
        // Hata olsa da devam et
      }
    }

    // Metin splitter baÅŸlat
    // Uzun metni daha kÃ¼Ã§Ã¼k parÃ§alara (chunk'lara) bÃ¶ler
    const splitter = new RecursiveCharacterTextSplitter({
      chunkSize: 1000,        // Her chunk maksimum 1000 karakter
      chunkOverlap: 200,      // Chunk'lar arasÄ±nda 200 karakter Ã¶rtÃ¼ÅŸme (context kaybÄ±nÄ± azaltmak iÃ§in)
    })
    
    // Her dosya iÃ§in ayrÄ± chunk'lama yap
    // BÃ¶ylece her dosyanÄ±n chunk'larÄ± 1'den baÅŸlayarak numaralanÄ±r
    const splitDocs: any[] = []
    // Dosya ismine gÃ¶re dokÃ¼mantlarÄ± gruplaÅŸtÄ±r
    const fileMap = new Map<string, any[]>()
    
    // allDocs'taki tÃ¼m dokÃ¼mantlarÄ± dosya ismine gÃ¶re grupla
    for (const { file, doc } of allDocs) {
      if (!fileMap.has(file)) fileMap.set(file, [])
      fileMap.get(file)!.push(doc)
    }
    
    // Her dosya grubunu ayrÄ± ayrÄ± chunk'la
    for (const [file, fileDocs] of fileMap.entries()) {
      // Bu dosyaya ait dokÃ¼mantlarÄ± chunk'la
      const fileChunks = await splitter.splitDocuments(fileDocs)
      // Her chunk'a 1'den baÅŸlayan numara ver (bu dosya iÃ§in)
      fileChunks.forEach((chunk, idx) => {
        chunk.metadata = { ...chunk.metadata, chunk: idx + 1 }
        splitDocs.push(chunk)
      })
    }

    // PostgreSQL'e kaydet
    // TÃ¼m chunk'larÄ± veritabanÄ±na insert et
    let insertedCount = 0
    
    // ğŸ†” Her dosya grubu iÃ§in file_id'yi bir kez belirle (BAÅI'NDA)
    // DÃ¶ngÃ¼ iÃ§inde MAX sorgusu Ã§alÄ±ÅŸtÄ±rÄ±rsan, her dÃ¶ngÃ¼de sonuÃ§ deÄŸiÅŸebilir
    const fileIdMap = new Map<string, number>()
    const maxFileIdResult = await pool.query(
      'SELECT COALESCE(MAX(file_id), 0) as max_file_id FROM documents WHERE user_id = $1',
      [userId]
    )
    let nextFileId = (maxFileIdResult.rows[0]?.max_file_id ?? 0) + 1
    
    for (const [file, ] of fileMap.entries()) {
      // Her dosyaya sÄ±rayla artan file_id ver
      fileIdMap.set(file, nextFileId)
      console.log(`ğŸ“ ${file}: file_id = ${nextFileId}`)
      nextFileId++  // Sonraki dosya iÃ§in ID'yi artÄ±r
    }
    
    for (let i = 0; i < splitDocs.length; i++) {
      const doc = splitDocs[i]
      // Metadata'dan bilgiler oku
      const baseMeta = (doc.metadata as any) || {}
      // PDF'in hangi sayfasÄ±ndan geldiÄŸini belirle
      const page = baseMeta.loc?.pageNumber ?? baseMeta.page ?? 'N/A'
      // SatÄ±r numarasÄ±nÄ± belirle (eÄŸer varsa)
      const lineNumber = baseMeta.loc?.lines?.from ?? 'N/A'
      // Chunk numarasÄ± daha Ã¶nce set edilmiÅŸ, onu koru
      const chunkNum = baseMeta.chunk || 'N/A'
      // Metadata'yÄ± hazÄ±rla
      doc.metadata = { ...baseMeta, chunk: chunkNum, page, lineNumber }
      
      try {
        // Bu chunk'Ä± embedding modeline gÃ¶nder (vektÃ¶r halinde kodla)
        // Ã–nce metni temizle: null karakterleri ve kontrol karakterlerini kaldÄ±r
        let cleanContent = doc.pageContent
          .replace(/\u0000/g, '')           // Null karakterleri kaldÄ±r
          .replace(/[\x00-\x1F\x7F]/g, '')  // Kontrol karakterlerini kaldÄ±r
          .replace(/[\uFEFF]/g, '')          // BOM karakterini kaldÄ±r
          .replace(/[^\x20-\x7E\xA0-\xFF]/g, '') // BaskÄ± yapÄ±lamayan karakterleri kaldÄ±r
          .trim();
        
        // BoÅŸ metin kontrol et
        if (!cleanContent) {
          console.warn('âš ï¸ Chunk temizlendikten sonra boÅŸ');
          continue;
        }
        
        const embedding = await embeddings.embedQuery(cleanContent)
        
        // Embedding baÅŸarÄ±lÄ± mÄ± kontrol et
        if (!embedding || embedding.length === 0) {
          console.error(' Embedding boÅŸ:', cleanContent.substring(0, 50))
          continue
        }

        console.log(' Embedding boyutu:', embedding.length, 'User:', userId)
        
        // Metadata'yÄ± da temizle (null karakterleri kaldÄ±r)
        let cleanMetadata = JSON.stringify(doc.metadata)
          .replace(/\u0000/g, '')
          .replace(/[\x00-\x1F\x7F]/g, '')
          .replace(/[\uFEFF]/g, '')
          .replace(/[^\x20-\x7E\xA0-\xFF]/g, '');
        
        // JSON geÃ§erliliÄŸini kontrol et
        try {
          JSON.parse(cleanMetadata);
        } catch (e) {
          console.warn('âš ï¸ Metadata JSON hata, temizleniyor');
          cleanMetadata = JSON.stringify({ source: doc.metadata?.source || 'unknown', chunk: doc.metadata?.chunk || 'N/A' });
        }
        
        // VeritabanÄ±na insert et
        // ğŸ†” file_id: Her dosya iÃ§in SABIT (tÃ¼m chunks bu ID'yi paylaÅŸÄ±r)
        // file_id dosya baÅŸÄ±nda belirlenmiÅŸ ve map'te tutuluyor
        const fileId = fileIdMap.get(doc.metadata.source) || 1
        const result = await pool.query(
          `INSERT INTO public.documents (user_id, content, metadata, embedding, file_id)
           VALUES ($1, $2, $3::jsonb, $4::vector, $5)
           RETURNING id, file_id`,
          [userId, cleanContent, cleanMetadata, JSON.stringify(embedding), fileId]
        )
        insertedCount++
        console.log(` Chunk kaydedildi: ID=${result.rows[0]?.id}, FileID=${result.rows[0]?.file_id}`)
      } catch (insertErr: any) {
        // EÄŸer bu chunk insert edilemezse, hata yaz ama devam et
        console.error(' Chunk insert hatasÄ±:', insertErr.message)
        console.error('   Error Code:', insertErr.code)
        console.error('   SQL:', insertErr.detail)
      }
    }

    // BaÅŸarÄ± cevabÄ± dÃ¶ndÃ¼r
    // KaÃ§ chunk'Ä±n baÅŸarÄ±yla kaydedildiÄŸini bildir
    console.log("\n" + "=".repeat(80));
    console.log("âœ… UPLOAD COMPLETE");
    console.log("=".repeat(80));
    console.log(`ğŸ“Š Toplam chunk: ${insertedCount}/${splitDocs.length}`);
    console.log(`ğŸ‘¤ UserID: ${userId}`);
    console.log(`ğŸ“ File groups: ${fileMap.size}`);
    for (const [file, ] of fileMap.entries()) {
      const fileId = fileIdMap.get(file);
      console.log(`   - ${file}: file_id=${fileId}`);
    }
    console.log("=".repeat(80) + "\n");
    
    return NextResponse.json({ 
      success: true, 
      count: insertedCount,
      message: `âœ… ${insertedCount}/${splitDocs.length} chunk baÅŸarÄ±yla kaydedildi`
    })
  } catch (err: any) {
    console.error('YÃ¼kleme hatasÄ±:', err)
    return NextResponse.json({ error: err.message || 'Ä°ÅŸleme hatasÄ±' }, { status: 500 })
  }
}
