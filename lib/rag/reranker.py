#!/usr/bin/env python3
"""
ğŸ¤– QWEN3 RERANKER MODULE
Qwen/Qwen3-Reranker-4B modelini kullanarak dokÃ¼mantasyonu query'ye gÃ¶re sÄ±ralar
Direct Python import olarak Ã§alÄ±ÅŸÄ±r (FastAPI server'a gerek yok)
"""

import torch
import torch.nn.functional as F
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from typing import List, Dict
import logging

# Logging ayarla
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global model ve tokenizer (lazy loading - ilk Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda yÃ¼klenir)
_model = None
_tokenizer = None
_device = None

def _load_model():
    """Model ve tokenizer'Ä± lazy load et"""
    global _model, _tokenizer, _device
    
    if _model is not None:
        return _model, _tokenizer, _device
    
    logger.info("ğŸ¤– Qwen3-Reranker-4B model yÃ¼kleniyor...")
    
    # Device seÃ§ (GPU varsa kullan)
    _device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ğŸ“ Device: {_device}")
    
    # Model ve tokenizer yÃ¼kle
    model_name = "Qwen/Qwen3-Reranker-4B"
    try:
        _tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        
        # Padding token'Ä±nÄ± ayarla - Qwen iÃ§in kritik!
        if _tokenizer.pad_token is None:
            _tokenizer.pad_token = "<|endoftext|>"  # Qwen pad token'Ä±
        
        logger.info(f"âœ… Tokenizer yÃ¼klendi (pad_token={_tokenizer.pad_token}, pad_token_id={_tokenizer.pad_token_id})")
        
        _model = AutoModelForSequenceClassification.from_pretrained(
            model_name, 
            trust_remote_code=True,
            torch_dtype=torch.float32,
            pad_token_id=_tokenizer.pad_token_id  # Kritik! Model'a pad_token_id'yi ver
        ).to(_device)
        _model.eval()  # Evaluation mode
        
        logger.info("âœ… Model baÅŸarÄ±yla yÃ¼klendi")
        return _model, _tokenizer, _device
        
    except Exception as e:
        logger.error(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
        raise

def rerank_documents(
    query: str,
    documents: List[str],
    top_k: int = 3
) -> List[Dict]:
    """
    Sorgu ve dokÃ¼manlara gÃ¶re rerank yapÄ±p en iyi sonuÃ§larÄ± dÃ¶ndÃ¼r
    
    Args:
        query: Arama sorgusu
        documents: DokÃ¼mantasyon listesi
        top_k: KaÃ§ tane dÃ¶ndÃ¼rÃ¼lecek (default: 3)
    
    Returns:
        List[Dict]: SÄ±ralanmÄ±ÅŸ dokÃ¼mantasyon
        [
            {
                "index": 0,
                "document": "...",
                "score": 0.95
            },
            ...
        ]
    """
    # Model ve tokenizer'Ä± yÃ¼kle
    model, tokenizer, device = _load_model()
    
    if not documents:
        logger.warning("âš ï¸ DokÃ¼mantasyon listesi boÅŸ")
        return []
    
    try:
        logger.info(f"ğŸ”„ Reranking baÅŸladÄ±: sorgu='{query[:50]}...', dokÃ¼={len(documents)}")
        
        # Her dokÃ¼mantÄ± sorgu ile pair yap
        pairs = [[query, doc] for doc in documents]
        
        # Tokenize et - daha kÄ±sa max_length ve padding kontrolÃ¼
        with torch.no_grad():
            inputs = tokenizer(
                pairs,
                padding=True,  # Dinamik padding
                truncation=True,
                return_tensors='pt',
                max_length=128  # Daha kÄ±sa - inference'Ä± hÄ±zlandÄ±r
            ).to(device)
            
            logger.info(f"   âœ… Tokenize baÅŸarÄ±lÄ±: input shape={inputs['input_ids'].shape}")
            logger.info(f"   ğŸ“ Input keys: {list(inputs.keys())}")
            logger.info(f"   ğŸ”§ Attention mask: {inputs['attention_mask'][0][:20]}...")
            
            # Model Ã§alÄ±ÅŸtÄ±r
            try:
                logger.info(f"   ğŸš€ Model inference baÅŸladÄ±...")
                # Explicit attention mask ve token_type_ids ekle
                model_inputs = {
                    'input_ids': inputs['input_ids'],
                    'attention_mask': inputs['attention_mask']
                }
                if 'token_type_ids' in inputs:
                    model_inputs['token_type_ids'] = inputs['token_type_ids']
                
                outputs = model(**model_inputs)
                logger.info(f"   âœ… Model inference baÅŸarÄ±lÄ±, logits shape={outputs.logits.shape}")
                
                # Logits shape: (batch_size, num_labels=2)
                # Label 0: not-relevant, Label 1: relevant
                # Logits'i softmax ile probability'ye dÃ¶nÃ¼ÅŸtÃ¼r
                logits = outputs.logits
                logger.info(f"   ğŸ“Š Logits sample: {logits[0].detach().cpu().tolist()}")
                
                probs = F.softmax(logits, dim=-1)  # Softmax
                logger.info(f"   ğŸ“Š Probs sample: {probs[0].detach().cpu().tolist()}")
                
                scores = probs[:, 1].cpu().tolist()  # Relevant class'Ä±n probability'si
                logger.info(f"   âœ… Scores hesaplandÄ±: {scores[:3]}...")
                
            except Exception as e:
                logger.error(f"   âŒ Model output hatasÄ±: {str(e)}")
                logger.error(f"      Exception type: {type(e).__name__}")
                import traceback
                logger.error(f"      Traceback: {traceback.format_exc()}")
                # Fallback: outputlar olduÄŸu gibi kullan
                scores = [float(i) for i in range(len(documents))]
                logger.warning(f"   âš ï¸ Fallback score'lar kullanÄ±lÄ±yor: {scores}")
        
        # Skor ile indeks pair yap
        scored_docs = [
            {
                "index": idx,
                "document": doc,
                "score": float(score)
            }
            for idx, (doc, score) in enumerate(zip(documents, scores))
        ]
        
        # Skor'a gÃ¶re azalan sÄ±rada sÄ±rala
        ranked = sorted(scored_docs, key=lambda x: x["score"], reverse=True)
        
        # Top K al
        top_k = min(top_k, len(ranked))
        ranked = ranked[:top_k]
        
        logger.info(f"âœ… Reranking tamamlandÄ±: top {top_k} seÃ§ildi")
        logger.info(f"   En yÃ¼ksek skor: {ranked[0]['score']:.4f}")
        
        return ranked
        
    except Exception as e:
        logger.error(f"âŒ Reranking hatasÄ±: {str(e)}")
        raise

if __name__ == "__main__":
    # Test
    query = "TÃ¼rkiye'nin baÅŸkenti neresi?"
    documents = [
        "Ankara, TÃ¼rkiye'nin baÅŸkenti. Ankara, Anadolu'nun ortasÄ±nda yer alÄ±r.",
        "Ä°stanbul, TÃ¼rkiye'nin en bÃ¼yÃ¼k ÅŸehridir.",
    ]
    
    result = rerank_documents(query, documents, top_k=2)
    print("\nâœ… Reranking Result:")
    for item in result:
        print(f"  [{item['index']}] Score: {item['score']:.4f} - {item['document'][:50]}...")
